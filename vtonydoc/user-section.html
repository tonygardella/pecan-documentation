<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>5 User Tutorial Section | The Predictive Ecosystem Analyzer</title>
  <meta name="description" content="5 User Tutorial Section | The Predictive Ecosystem Analyzer">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="5 User Tutorial Section | The Predictive Ecosystem Analyzer" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 User Tutorial Section | The Predictive Ecosystem Analyzer" />
  
  
  

<meta name="author" content="By: PEcAn Team">


<meta name="date" content="2019-04-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="pecan-manual-setup.html">
<link rel="next" href="developer-guide.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="project-overview.html"><a href="project-overview.html"><i class="fa fa-check"></i><b>1</b> Project Overview</a></li>
<li class="chapter" data-level="2" data-path="contributor-covenant-code-of-conduct.html"><a href="contributor-covenant-code-of-conduct.html"><i class="fa fa-check"></i><b>2</b> Contributor Covenant Code of Conduct</a></li>
<li class="chapter" data-level="3" data-path="about-the-pecan-book.html"><a href="about-the-pecan-book.html"><i class="fa fa-check"></i><b>3</b> About the PEcAn Book</a><ul>
<li class="chapter" data-level="3.0.1" data-path="about-the-pecan-book.html"><a href="about-the-pecan-book.html#general-feedbackcommentssuggestions"><i class="fa fa-check"></i><b>3.0.1</b> General Feedback/Comments/Suggestions</a></li>
<li class="chapter" data-level="3.1" data-path="about-the-pecan-book.html"><a href="about-the-pecan-book.html#bookediting"><i class="fa fa-check"></i><b>3.1</b> Editing this book</a></li>
</ul></li>
<li class="part"><span><b>II Tutorials,Demos and How To’s</b></span></li>
<li class="chapter" data-level="4" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html"><i class="fa fa-check"></i><b>4</b> Install PEcAn</a><ul>
<li class="chapter" data-level="4.1" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#pecanvm"><i class="fa fa-check"></i><b>4.1</b> PEcAn Virtual Machine</a><ul>
<li class="chapter" data-level="4.1.1" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#maintain-vm"><i class="fa fa-check"></i><b>4.1.1</b> Maintaining your PEcAn VM</a></li>
<li class="chapter" data-level="4.1.2" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#working-with-vm"><i class="fa fa-check"></i><b>4.1.2</b> Working with the VM</a></li>
<li class="chapter" data-level="4.1.3" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#awsvm"><i class="fa fa-check"></i><b>4.1.3</b> Using Amazon Web Services for a VM (AWS)</a></li>
<li class="chapter" data-level="4.1.4" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#createvm"><i class="fa fa-check"></i><b>4.1.4</b> Creating a Virtual Machine</a></li>
<li class="chapter" data-level="4.1.5" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#vm-dektop-conversion"><i class="fa fa-check"></i><b>4.1.5</b> VM Desktop Conversion</a></li>
<li class="chapter" data-level="4.1.6" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#aws-setup"><i class="fa fa-check"></i><b>4.1.6</b> AWS Setup</a></li>
<li class="chapter" data-level="4.1.7" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#porting-vm-to-aws"><i class="fa fa-check"></i><b>4.1.7</b> Porting VM to AWS</a></li>
<li class="chapter" data-level="4.1.8" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#set-up-multiple-instances-optional"><i class="fa fa-check"></i><b>4.1.8</b> Set up multiple instances (optional)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#shiny-setup"><i class="fa fa-check"></i><b>4.2</b> Shiny Setup</a><ul>
<li class="chapter" data-level="4.2.1" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#install-the-shiny-r-package-and-shiny-server"><i class="fa fa-check"></i><b>4.2.1</b> Install the Shiny R package and Shiny server</a></li>
<li class="chapter" data-level="4.2.2" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#modify-the-shiny-configuration-file"><i class="fa fa-check"></i><b>4.2.2</b> Modify the shiny configuration file</a></li>
<li class="chapter" data-level="4.2.3" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#set-the-apache-proxy"><i class="fa fa-check"></i><b>4.2.3</b> Set the Apache proxy</a></li>
<li class="chapter" data-level="4.2.4" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#enable-and-start-the-shiny-server-and-restart-apache"><i class="fa fa-check"></i><b>4.2.4</b> Enable and start the shiny server, and restart apache</a></li>
<li class="chapter" data-level="4.2.5" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#troubleshooting"><i class="fa fa-check"></i><b>4.2.5</b> Troubleshooting</a></li>
<li class="chapter" data-level="4.2.6" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#further-reading"><i class="fa fa-check"></i><b>4.2.6</b> Further reading</a></li>
<li class="chapter" data-level="4.2.7" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#thredds-setup"><i class="fa fa-check"></i><b>4.2.7</b> Thredds Setup</a></li>
<li class="chapter" data-level="4.2.8" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#install-the-tomcat-8-and-thredds-webapp"><i class="fa fa-check"></i><b>4.2.8</b> Install the Tomcat 8 and Thredds webapp</a></li>
<li class="chapter" data-level="4.2.9" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#update-the-catalog"><i class="fa fa-check"></i><b>4.2.9</b> Update the catalog</a></li>
<li class="chapter" data-level="4.2.10" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#troubleshooting-1"><i class="fa fa-check"></i><b>4.2.10</b> Troubleshooting</a></li>
<li class="chapter" data-level="4.2.11" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#further-reading-1"><i class="fa fa-check"></i><b>4.2.11</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#docker-index"><i class="fa fa-check"></i><b>4.3</b> Docker</a><ul>
<li class="chapter" data-level="4.3.1" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#setup-pecan-using-docker-compose"><i class="fa fa-check"></i><b>4.3.1</b> Setup PEcAn using docker-compose</a></li>
<li class="chapter" data-level="4.3.2" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#pecan-docker-quickstart-init"><i class="fa fa-check"></i><b>4.3.2</b> Initialize PEcAn (first time only)</a></li>
<li class="chapter" data-level="4.3.3" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#docker-quickstart-troubleshooting"><i class="fa fa-check"></i><b>4.3.3</b> Troubleshooting</a></li>
<li class="chapter" data-level="4.3.4" data-path="pecan-manual-setup.html"><a href="pecan-manual-setup.html#osinstall"><i class="fa fa-check"></i><b>4.3.4</b> OS Specific Installations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="user-section.html"><a href="user-section.html"><i class="fa fa-check"></i><b>5</b> User Tutorial Section</a><ul>
<li class="chapter" data-level="5.0.1" data-path="user-section.html"><a href="user-section.html#demo-table"><i class="fa fa-check"></i><b>5.0.1</b> PEcAn Demos</a></li>
<li class="chapter" data-level="5.1" data-path="user-section.html"><a href="user-section.html#basic-web-wrokflow"><i class="fa fa-check"></i><b>5.1</b> Basic Web workflow</a><ul>
<li class="chapter" data-level="5.1.1" data-path="user-section.html"><a href="user-section.html#web-site-model"><i class="fa fa-check"></i><b>5.1.1</b> Site and model selection</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="user-section.html"><a href="user-section.html#intermediate-user"><i class="fa fa-check"></i><b>5.2</b> PEcAn Web Interface</a></li>
<li class="chapter" data-level="5.3" data-path="user-section.html"><a href="user-section.html#advanced-user"><i class="fa fa-check"></i><b>5.3</b> Advanced User Guide</a><ul>
<li class="chapter" data-level="5.3.1" data-path="user-section.html"><a href="user-section.html#adding-to-pecan"><i class="fa fa-check"></i><b>5.3.1</b> Adding to PEcAn</a></li>
<li class="chapter" data-level="5.3.2" data-path="user-section.html"><a href="user-section.html#web-curl-submission"><i class="fa fa-check"></i><b>5.3.2</b> Submitting Workflow from Command Line</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="developer-guide.html"><a href="developer-guide.html"><i class="fa fa-check"></i><b>6</b> Developer guide</a><ul>
<li class="chapter" data-level="6.1" data-path="developer-guide.html"><a href="developer-guide.html#updatebety"><i class="fa fa-check"></i><b>6.1</b> Updating PEcAn Code and Bety Database</a></li>
<li class="chapter" data-level="6.2" data-path="developer-guide.html"><a href="developer-guide.html#pecan-git"><i class="fa fa-check"></i><b>6.2</b> Git and GitHub Workflow</a><ul>
<li class="chapter" data-level="6.2.1" data-path="developer-guide.html"><a href="developer-guide.html#using-git"><i class="fa fa-check"></i><b>6.2.1</b> Using Git</a></li>
<li class="chapter" data-level="6.2.2" data-path="developer-guide.html"><a href="developer-guide.html#github-use-with-pecan"><i class="fa fa-check"></i><b>6.2.2</b> GitHub use with PEcAn</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="developer-guide.html"><a href="developer-guide.html#coding-practices"><i class="fa fa-check"></i><b>6.3</b> Coding Practices</a></li>
<li class="chapter" data-level="6.4" data-path="developer-guide.html"><a href="developer-guide.html#coding-style"><i class="fa fa-check"></i><b>6.4</b> Coding Style</a></li>
<li class="chapter" data-level="6.5" data-path="developer-guide.html"><a href="developer-guide.html#logging"><i class="fa fa-check"></i><b>6.5</b> Logging</a><ul>
<li class="chapter" data-level="6.5.1" data-path="developer-guide.html"><a href="developer-guide.html#pecan-logging-functions"><i class="fa fa-check"></i><b>6.5.1</b> PEcAn logging functions</a></li>
<li class="chapter" data-level="6.5.2" data-path="developer-guide.html"><a href="developer-guide.html#other-r-logging-packages"><i class="fa fa-check"></i><b>6.5.2</b> Other R logging packages</a></li>
<li class="chapter" data-level="6.5.3" data-path="developer-guide.html"><a href="developer-guide.html#example-usage"><i class="fa fa-check"></i><b>6.5.3</b> Example Usage</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="developer-guide.html"><a href="developer-guide.html#package-data"><i class="fa fa-check"></i><b>6.6</b> Package Data</a></li>
<li class="chapter" data-level="6.7" data-path="developer-guide.html"><a href="developer-guide.html#roxygen2-1"><i class="fa fa-check"></i><b>6.7</b> Roxygen2</a></li>
<li class="chapter" data-level="6.8" data-path="developer-guide.html"><a href="developer-guide.html#testing"><i class="fa fa-check"></i><b>6.8</b> Testing</a></li>
<li class="chapter" data-level="6.9" data-path="developer-guide.html"><a href="developer-guide.html#download-and-compile-pecan"><i class="fa fa-check"></i><b>6.9</b> Download and Compile PEcAn</a><ul>
<li class="chapter" data-level="6.9.1" data-path="developer-guide.html"><a href="developer-guide.html#pecan-testrun"><i class="fa fa-check"></i><b>6.9.1</b> PEcAn Testrun</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="developer-guide.html"><a href="developer-guide.html#directory-structure"><i class="fa fa-check"></i><b>6.10</b> Directory structure</a></li>
</ul></li>
<li class="part"><span><b>III Topical Pages</b></span></li>
<li class="chapter" data-level="7" data-path="pecan-standards.html"><a href="pecan-standards.html"><i class="fa fa-check"></i><b>7</b> PEcAn standard formats</a><ul>
<li class="chapter" data-level="7.1" data-path="pecan-standards.html"><a href="pecan-standards.html#defining-new-input-formats"><i class="fa fa-check"></i><b>7.1</b> Defining new input formats</a></li>
<li class="chapter" data-level="7.2" data-path="pecan-standards.html"><a href="pecan-standards.html#time-standard"><i class="fa fa-check"></i><b>7.2</b> Time Standard</a><ul>
<li class="chapter" data-level="7.2.1" data-path="pecan-standards.html"><a href="pecan-standards.html#input-standards"><i class="fa fa-check"></i><b>7.2.1</b> Input Standards</a></li>
<li class="chapter" data-level="7.2.2" data-path="pecan-standards.html"><a href="pecan-standards.html#soils-and-vegetation-inputs"><i class="fa fa-check"></i><b>7.2.2</b> Soils and Vegetation Inputs</a></li>
<li class="chapter" data-level="7.2.3" data-path="pecan-standards.html"><a href="pecan-standards.html#output-standards"><i class="fa fa-check"></i><b>7.2.3</b> Output Standards</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="pecanXML.html"><a href="pecanXML.html"><i class="fa fa-check"></i><b>8</b> The PEcAn XML</a><ul>
<li class="chapter" data-level="8.0.1" data-path="pecanXML.html"><a href="pecanXML.html#xml-core-config"><i class="fa fa-check"></i><b>8.0.1</b> Core configuration</a></li>
<li class="chapter" data-level="8.0.2" data-path="pecanXML.html"><a href="pecanXML.html#xml-outdir"><i class="fa fa-check"></i><b>8.0.2</b> <code>outdir</code>: Output directory</a></li>
<li class="chapter" data-level="8.0.3" data-path="pecanXML.html"><a href="pecanXML.html#xml-database"><i class="fa fa-check"></i><b>8.0.3</b> <code>database</code>: PEcAn database settings</a></li>
<li class="chapter" data-level="8.0.4" data-path="pecanXML.html"><a href="pecanXML.html#xml-pft"><i class="fa fa-check"></i><b>8.0.4</b> <code>pft</code>: Plant functional type selection</a></li>
<li class="chapter" data-level="8.0.5" data-path="pecanXML.html"><a href="pecanXML.html#xml-meta-analysis"><i class="fa fa-check"></i><b>8.0.5</b> <code>meta.analysis</code>: Trait Meta Analysis</a></li>
<li class="chapter" data-level="8.0.6" data-path="pecanXML.html"><a href="pecanXML.html#xml-model"><i class="fa fa-check"></i><b>8.0.6</b> <code>model</code>: Model configuration</a></li>
<li class="chapter" data-level="8.0.7" data-path="pecanXML.html"><a href="pecanXML.html#xml-run"><i class="fa fa-check"></i><b>8.0.7</b> <code>run</code>: Run Setup</a></li>
<li class="chapter" data-level="8.0.8" data-path="pecanXML.html"><a href="pecanXML.html#xml-host"><i class="fa fa-check"></i><b>8.0.8</b> <code>host</code>: Host information for remote execution</a></li>
<li class="chapter" data-level="8.0.9" data-path="pecanXML.html"><a href="pecanXML.html#xml-advanced"><i class="fa fa-check"></i><b>8.0.9</b> Advanced features</a></li>
<li class="chapter" data-level="8.0.10" data-path="pecanXML.html"><a href="pecanXML.html#xml-ensemble"><i class="fa fa-check"></i><b>8.0.10</b> <code>ensemble</code>: Ensemble Runs</a></li>
<li class="chapter" data-level="8.0.11" data-path="pecanXML.html"><a href="pecanXML.html#xml-sensitivity-analysis"><i class="fa fa-check"></i><b>8.0.11</b> <code>sensitivity.analysis</code>: Sensitivity analysis</a></li>
<li class="chapter" data-level="8.0.12" data-path="pecanXML.html"><a href="pecanXML.html#xml-parameter-data-assimilation"><i class="fa fa-check"></i><b>8.0.12</b> Parameter Data Assimilation</a></li>
<li class="chapter" data-level="8.0.13" data-path="pecanXML.html"><a href="pecanXML.html#xml-multi-settings"><i class="fa fa-check"></i><b>8.0.13</b> Multi-Settings</a></li>
<li class="chapter" data-level="8.0.14" data-path="pecanXML.html"><a href="pecanXML.html#xml-state-data-assimilation"><i class="fa fa-check"></i><b>8.0.14</b> (experimental) State Data Assimilation</a></li>
<li class="chapter" data-level="8.0.15" data-path="pecanXML.html"><a href="pecanXML.html#xml-browndog"><i class="fa fa-check"></i><b>8.0.15</b> (experimental) Brown Dog</a></li>
<li class="chapter" data-level="8.0.16" data-path="pecanXML.html"><a href="pecanXML.html#xml-benchmarking"><i class="fa fa-check"></i><b>8.0.16</b> (experimental) Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>9</b> PEcAn workflow (web/workflow.R)</a><ul>
<li class="chapter" data-level="9.0.1" data-path="workflow.html"><a href="workflow.html#workflow-readsettings"><i class="fa fa-check"></i><b>9.0.1</b> Read Settings</a></li>
<li class="chapter" data-level="9.0.2" data-path="workflow.html"><a href="workflow.html#workflow-input"><i class="fa fa-check"></i><b>9.0.2</b> Input Conversions</a></li>
<li class="chapter" data-level="9.0.3" data-path="workflow.html"><a href="workflow.html#workflow-input-data"><i class="fa fa-check"></i><b>9.0.3</b> Input Data</a></li>
<li class="chapter" data-level="9.0.4" data-path="workflow.html"><a href="workflow.html#workflow-input-initial"><i class="fa fa-check"></i><b>9.0.4</b> Initial Conditions</a></li>
<li class="chapter" data-level="9.0.5" data-path="workflow.html"><a href="workflow.html#workflow-met"><i class="fa fa-check"></i><b>9.0.5</b> Meteorological Data</a></li>
<li class="chapter" data-level="9.0.6" data-path="workflow.html"><a href="workflow.html#workflow-met-standard"><i class="fa fa-check"></i><b>9.0.6</b> Converting raw data to PEcAn standard</a></li>
<li class="chapter" data-level="9.0.7" data-path="workflow.html"><a href="workflow.html#workflow-met-downscale"><i class="fa fa-check"></i><b>9.0.7</b> Downscaling and gapfilling (optional)</a></li>
<li class="chapter" data-level="9.0.8" data-path="workflow.html"><a href="workflow.html#workflow-met-model"><i class="fa fa-check"></i><b>9.0.8</b> Converting from PEcAn standard to model-specific format</a></li>
<li class="chapter" data-level="9.0.9" data-path="workflow.html"><a href="workflow.html#workflow-traits"><i class="fa fa-check"></i><b>9.0.9</b> Traits</a></li>
<li class="chapter" data-level="9.0.10" data-path="workflow.html"><a href="workflow.html#workflow-metaanalysis"><i class="fa fa-check"></i><b>9.0.10</b> Meta Analysis</a></li>
<li class="chapter" data-level="9.0.11" data-path="workflow.html"><a href="workflow.html#workflow-modelconfig"><i class="fa fa-check"></i><b>9.0.11</b> Model Configuration</a></li>
<li class="chapter" data-level="9.0.12" data-path="workflow.html"><a href="workflow.html#workflow-modelrun"><i class="fa fa-check"></i><b>9.0.12</b> Run Execution</a></li>
<li class="chapter" data-level="9.0.13" data-path="workflow.html"><a href="workflow.html#workflow-postrun"><i class="fa fa-check"></i><b>9.0.13</b> Post Run Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pecan-models.html"><a href="pecan-models.html"><i class="fa fa-check"></i><b>10</b> PEcAn Models</a><ul>
<li class="chapter" data-level="10.0.1" data-path="pecan-models.html"><a href="pecan-models.html#models-biocro"><i class="fa fa-check"></i><b>10.0.1</b> BioCro</a></li>
<li class="chapter" data-level="10.0.2" data-path="pecan-models.html"><a href="pecan-models.html#models-clm"><i class="fa fa-check"></i><b>10.0.2</b> CLM</a></li>
<li class="chapter" data-level="10.0.3" data-path="pecan-models.html"><a href="pecan-models.html#models-dalec"><i class="fa fa-check"></i><b>10.0.3</b> DALEC</a></li>
<li class="chapter" data-level="10.0.4" data-path="pecan-models.html"><a href="pecan-models.html#models-ed"><i class="fa fa-check"></i><b>10.0.4</b> ED2</a></li>
<li class="chapter" data-level="10.0.5" data-path="pecan-models.html"><a href="pecan-models.html#installation-notes"><i class="fa fa-check"></i><b>10.0.5</b> Installation notes</a></li>
<li class="chapter" data-level="10.0.6" data-path="pecan-models.html"><a href="pecan-models.html#models-gday"><i class="fa fa-check"></i><b>10.0.6</b> GDAY</a></li>
<li class="chapter" data-level="10.0.7" data-path="pecan-models.html"><a href="pecan-models.html#models-linkages"><i class="fa fa-check"></i><b>10.0.7</b> LINKAGES</a></li>
<li class="chapter" data-level="10.0.8" data-path="pecan-models.html"><a href="pecan-models.html#models-lpjguess"><i class="fa fa-check"></i><b>10.0.8</b> LPJ-GUESS</a></li>
<li class="chapter" data-level="10.0.9" data-path="pecan-models.html"><a href="pecan-models.html#models-maespa"><i class="fa fa-check"></i><b>10.0.9</b> MAESPA</a></li>
<li class="chapter" data-level="10.0.10" data-path="pecan-models.html"><a href="pecan-models.html#models-preles"><i class="fa fa-check"></i><b>10.0.10</b> PRELES</a></li>
<li class="chapter" data-level="10.0.11" data-path="pecan-models.html"><a href="pecan-models.html#models-sipnet"><i class="fa fa-check"></i><b>10.0.11</b> SiPNET</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html"><i class="fa fa-check"></i><b>11</b> Available Meteorological Drivers</a><ul>
<li class="chapter" data-level="11.0.1" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#ameriflux"><i class="fa fa-check"></i><b>11.0.1</b> Ameriflux</a></li>
<li class="chapter" data-level="11.0.2" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#amerifluxlbl"><i class="fa fa-check"></i><b>11.0.2</b> AmerifluxLBL</a></li>
<li class="chapter" data-level="11.0.3" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#fluxnet2015"><i class="fa fa-check"></i><b>11.0.3</b> Fluxnet2015</a></li>
<li class="chapter" data-level="11.0.4" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#narr"><i class="fa fa-check"></i><b>11.0.4</b> NARR</a></li>
<li class="chapter" data-level="11.0.5" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#cruncep"><i class="fa fa-check"></i><b>11.0.5</b> CRUNCEP</a></li>
<li class="chapter" data-level="11.0.6" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#cmip5"><i class="fa fa-check"></i><b>11.0.6</b> CMIP5</a></li>
<li class="chapter" data-level="11.0.7" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#nldas"><i class="fa fa-check"></i><b>11.0.7</b> NLDAS</a></li>
<li class="chapter" data-level="11.0.8" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#gldas"><i class="fa fa-check"></i><b>11.0.8</b> GLDAS</a></li>
<li class="chapter" data-level="11.0.9" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#paleon"><i class="fa fa-check"></i><b>11.0.9</b> PalEON</a></li>
<li class="chapter" data-level="11.0.10" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#fluxnetlathuile"><i class="fa fa-check"></i><b>11.0.10</b> FluxnetLaThuile</a></li>
<li class="chapter" data-level="11.0.11" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#geostreams"><i class="fa fa-check"></i><b>11.0.11</b> Geostreams</a></li>
<li class="chapter" data-level="11.0.12" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#download-gfdl"><i class="fa fa-check"></i><b>11.0.12</b> Download GFDL</a></li>
<li class="chapter" data-level="11.0.13" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#cm3"><i class="fa fa-check"></i><b>11.0.13</b> CM3</a></li>
<li class="chapter" data-level="11.0.14" data-path="available-meteorological-drivers.html"><a href="available-meteorological-drivers.html#esm2m-esm2g"><i class="fa fa-check"></i><b>11.0.14</b> ESM2M &amp; ESM2G</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pecan-api.html"><a href="pecan-api.html"><i class="fa fa-check"></i><b>12</b> The PEcAn Docker API</a></li>
<li class="chapter" data-level="13" data-path="database-synchronization.html"><a href="database-synchronization.html"><i class="fa fa-check"></i><b>13</b> Database synchronization</a><ul>
<li class="chapter" data-level="13.0.1" data-path="database-synchronization.html"><a href="database-synchronization.html#how-does-it-work"><i class="fa fa-check"></i><b>13.0.1</b> How does it work?</a></li>
<li class="chapter" data-level="13.0.2" data-path="database-synchronization.html"><a href="database-synchronization.html#set-up"><i class="fa fa-check"></i><b>13.0.2</b> Set up</a></li>
<li class="chapter" data-level="13.0.3" data-path="database-synchronization.html"><a href="database-synchronization.html#fetch-latest-data"><i class="fa fa-check"></i><b>13.0.3</b> Fetch latest data</a></li>
<li class="chapter" data-level="13.0.4" data-path="database-synchronization.html"><a href="database-synchronization.html#sharing-data"><i class="fa fa-check"></i><b>13.0.4</b> Sharing data</a></li>
<li class="chapter" data-level="13.0.5" data-path="database-synchronization.html"><a href="database-synchronization.html#automation"><i class="fa fa-check"></i><b>13.0.5</b> Automation</a></li>
<li class="chapter" data-level="13.0.6" data-path="database-synchronization.html"><a href="database-synchronization.html#database-maintentance"><i class="fa fa-check"></i><b>13.0.6</b> Database maintentance</a></li>
<li class="chapter" data-level="13.0.7" data-path="database-synchronization.html"><a href="database-synchronization.html#troubleshooting-3"><i class="fa fa-check"></i><b>13.0.7</b> Troubleshooting</a></li>
<li class="chapter" data-level="13.0.8" data-path="database-synchronization.html"><a href="database-synchronization.html#network-status-map"><i class="fa fa-check"></i><b>13.0.8</b> Network Status Map</a></li>
<li class="chapter" data-level="13.0.9" data-path="database-synchronization.html"><a href="database-synchronization.html#tasks"><i class="fa fa-check"></i><b>13.0.9</b> Tasks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="standalone-tools-modules.html"><a href="standalone-tools-modules.html"><i class="fa fa-check"></i><b>14</b> Standalone tools (modules)</a><ul>
<li class="chapter" data-level="14.0.1" data-path="standalone-tools-modules.html"><a href="standalone-tools-modules.html#LoadData"><i class="fa fa-check"></i><b>14.0.1</b> Loading Data in PEcAn</a></li>
<li class="chapter" data-level="14.0.2" data-path="standalone-tools-modules.html"><a href="standalone-tools-modules.html#function-load_data"><i class="fa fa-check"></i><b>14.0.2</b> Function <code>load_data</code></a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shiny.html"><a href="shiny.html"><i class="fa fa-check"></i><b>15</b> SHINY</a><ul>
<li class="chapter" data-level="15.0.1" data-path="shiny.html"><a href="shiny.html#debugging-shiny-apps"><i class="fa fa-check"></i><b>15.0.1</b> Debugging Shiny Apps</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="debugging.html"><a href="debugging.html"><i class="fa fa-check"></i><b>16</b> Debugging</a><ul>
<li class="chapter" data-level="16.0.1" data-path="debugging.html"><a href="debugging.html#using-testsworkflow.r"><i class="fa fa-check"></i><b>16.0.1</b> Using <code>tests/workflow.R</code></a></li>
<li class="chapter" data-level="16.0.2" data-path="debugging.html"><a href="debugging.html#useful-scripts"><i class="fa fa-check"></i><b>16.0.2</b> Useful scripts</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="troubleshooting-pecan.html"><a href="troubleshooting-pecan.html"><i class="fa fa-check"></i><b>17</b> Troubleshooting PEcAn</a><ul>
<li class="chapter" data-level="17.0.1" data-path="troubleshooting-pecan.html"><a href="troubleshooting-pecan.html#cookies-and-pecan-web-pages"><i class="fa fa-check"></i><b>17.0.1</b> Cookies and pecan web pages</a></li>
<li class="chapter" data-level="17.0.2" data-path="troubleshooting-pecan.html"><a href="troubleshooting-pecan.html#warning-mkdir-function.mkdir-no-such-file-or-directory"><i class="fa fa-check"></i><b>17.0.2</b> <code>Warning: mkdir() [function.mkdir]: No such file or directory</code></a></li>
<li class="chapter" data-level="17.0.3" data-path="troubleshooting-pecan.html"><a href="troubleshooting-pecan.html#after-creating-a-new-pft-the-tag-for-pft-not-passed-to-config.xml-in-ed"><i class="fa fa-check"></i><b>17.0.3</b> After creating a new PFT the <num> tag for PFT not passed to config.xml in ED</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="rabbitmq.html"><a href="rabbitmq.html"><i class="fa fa-check"></i><b>18</b> RabbitMQ</a><ul>
<li class="chapter" data-level="18.0.1" data-path="rabbitmq.html"><a href="rabbitmq.html#rabbitmq-basics-sender"><i class="fa fa-check"></i><b>18.0.1</b> Producer – <code>sender.py</code></a></li>
<li class="chapter" data-level="18.0.2" data-path="rabbitmq.html"><a href="rabbitmq.html#rabbitmq-basics-receiver"><i class="fa fa-check"></i><b>18.0.2</b> Consumer – <code>receiver.py</code></a></li>
<li class="chapter" data-level="18.0.3" data-path="rabbitmq.html"><a href="rabbitmq.html#rabbitmq-web"><i class="fa fa-check"></i><b>18.0.3</b> RabbitMQ and the PEcAn web interface</a></li>
<li class="chapter" data-level="18.0.4" data-path="rabbitmq.html"><a href="rabbitmq.html#rabbitmq-xml"><i class="fa fa-check"></i><b>18.0.4</b> RabbitMQ in the PEcAn XML</a></li>
<li class="chapter" data-level="18.0.5" data-path="rabbitmq.html"><a href="rabbitmq.html#rabbitmq-dockerfile"><i class="fa fa-check"></i><b>18.0.5</b> RabbitMQ configuration in Dockerfiles</a></li>
<li class="chapter" data-level="18.0.6" data-path="rabbitmq.html"><a href="rabbitmq.html#rabbitmq-case-study"><i class="fa fa-check"></i><b>18.0.6</b> Case study: PEcAn web interface</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="database.html"><a href="database.html"><i class="fa fa-check"></i><b>19</b> BETY Database Administration</a><ul>
<li class="chapter" data-level="19.0.1" data-path="database.html"><a href="database.html#database-setup"><i class="fa fa-check"></i><b>19.0.1</b> Best practices</a></li>
<li class="chapter" data-level="19.0.2" data-path="database.html"><a href="database.html#backup-of-bety-database"><i class="fa fa-check"></i><b>19.0.2</b> Backup of BETY database</a></li>
<li class="chapter" data-level="19.0.3" data-path="database.html"><a href="database.html#restore-of-bety-database"><i class="fa fa-check"></i><b>19.0.3</b> Restore of BETY database</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="workflow-modules.html"><a href="workflow-modules.html"><i class="fa fa-check"></i><b>20</b> Workflow modules</a><ul>
<li class="chapter" data-level="20.0.1" data-path="workflow-modules.html"><a href="workflow-modules.html#overview"><i class="fa fa-check"></i><b>20.0.1</b> Overview</a></li>
<li class="chapter" data-level="20.0.2" data-path="workflow-modules.html"><a href="workflow-modules.html#load-settings"><i class="fa fa-check"></i><b>20.0.2</b> Load Settings:</a></li>
<li class="chapter" data-level="20.0.3" data-path="workflow-modules.html"><a href="workflow-modules.html#query-database"><i class="fa fa-check"></i><b>20.0.3</b> Query Database:</a></li>
<li class="chapter" data-level="20.0.4" data-path="workflow-modules.html"><a href="workflow-modules.html#meta-analysis"><i class="fa fa-check"></i><b>20.0.4</b> Meta Analysis:</a></li>
<li class="chapter" data-level="20.0.5" data-path="workflow-modules.html"><a href="workflow-modules.html#write-configuration-files"><i class="fa fa-check"></i><b>20.0.5</b> Write Configuration Files</a></li>
<li class="chapter" data-level="20.0.6" data-path="workflow-modules.html"><a href="workflow-modules.html#start-runs"><i class="fa fa-check"></i><b>20.0.6</b> Start Runs:</a></li>
<li class="chapter" data-level="20.0.7" data-path="workflow-modules.html"><a href="workflow-modules.html#get-model-output"><i class="fa fa-check"></i><b>20.0.7</b> Get Model Output</a></li>
<li class="chapter" data-level="20.0.8" data-path="workflow-modules.html"><a href="workflow-modules.html#ensemble-analysis"><i class="fa fa-check"></i><b>20.0.8</b> Ensemble Analysis</a></li>
<li class="chapter" data-level="20.0.9" data-path="workflow-modules.html"><a href="workflow-modules.html#sensitivity-analysis-variance-decomposition"><i class="fa fa-check"></i><b>20.0.9</b> Sensitivity Analysis, Variance Decomposition</a></li>
<li class="chapter" data-level="20.0.10" data-path="workflow-modules.html"><a href="workflow-modules.html#glossary"><i class="fa fa-check"></i><b>20.0.10</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="data-assimilation-with-dart.html"><a href="data-assimilation-with-dart.html"><i class="fa fa-check"></i><b>21</b> Data assimilation with DART</a><ul>
<li class="chapter" data-level="21.1" data-path="data-assimilation-with-dart.html"><a href="data-assimilation-with-dart.html#using-the-pecan-download.file-function"><i class="fa fa-check"></i><b>21.1</b> Using the PEcAn download.file() function</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="pecan-project-use-to-teach-ecological-model-data-synthesis.html"><a href="pecan-project-use-to-teach-ecological-model-data-synthesis.html"><i class="fa fa-check"></i><b>22</b> PEcAn Project use to teach Ecological model-data synthesis</a><ul>
<li class="chapter" data-level="22.0.1" data-path="pecan-project-use-to-teach-ecological-model-data-synthesis.html"><a href="pecan-project-use-to-teach-ecological-model-data-synthesis.html#university-classes"><i class="fa fa-check"></i><b>22.0.1</b> University classes</a></li>
<li class="chapter" data-level="22.0.2" data-path="pecan-project-use-to-teach-ecological-model-data-synthesis.html"><a href="pecan-project-use-to-teach-ecological-model-data-synthesis.html#summer-courses-workshops"><i class="fa fa-check"></i><b>22.0.2</b> Summer Courses / Workshops</a></li>
<li class="chapter" data-level="22.0.3" data-path="pecan-project-use-to-teach-ecological-model-data-synthesis.html"><a href="pecan-project-use-to-teach-ecological-model-data-synthesis.html#selected-publications"><i class="fa fa-check"></i><b>22.0.3</b> Selected Publications</a></li>
</ul></li>
<li class="part"><span><b>IV Appendix</b></span><ul>
<li class="chapter" data-level="22.1" data-path="02_demos_tutorials_workflows/02_user_demos/01_introductions_user.html"><a href="#todo"><i class="fa fa-check"></i><b>22.1</b> TODO:</a></li>
<li class="chapter" data-level="22.2" data-path="pecan-project-use-to-teach-ecological-model-data-synthesis.html"><a href="pecan-project-use-to-teach-ecological-model-data-synthesis.html#faq"><i class="fa fa-check"></i><b>22.2</b> FAQ</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Predictive Ecosystem Analyzer</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="user-section" class="section level1">
<h1><span class="header-section-number">5</span> User Tutorial Section</h1>
<p>The user Section contains the following sections:
<a href="user-section.html#basic-web-wrokflow">Basic Web Workflow Usage</a>
<a href="#intermediate%20User%20Guide">PEcAn Web Interface</a>
<a href="user-section.html#advanced-user">PEcAn from the Command Line</a></p>
<div id="how-pecan-works-in-a-nutshell" class="section level4">
<h4><span class="header-section-number">5.0.0.1</span> How PEcAn Works in a nutshell</h4>
<p>PEcAn provides an interface to a variety of ecosystem models and attempts to standardize and automate the processes of model parameterization, execution, and analysis. First, you choose an ecosystem model, then the time and location of interest (a site), the plant community (or crop) that you are interested in simulating, and a source of atmospheric data from the BETY database (LeBauer et al, 2010). These are set in a “settings” file, commonly named <code>pecan.xml</code> which can be edited manually if desired. From here, PEcAn will take over and set up and execute the selected model using your settings. The key is that PEcAn uses models as-is, and all of the translation steps are done within PEcAn so no modifications are required of the model itself. Once the model is finished it will allow you to create graphs with the results of the simulation as well as download the results. It is also possible to see all past experiments and simulations.</p>
<p>There are two ways of using PEcAn, via the web interface and directly within R. Even for users familiar with R, using the web interface is a good place to start because it provides a high level overview of the PEcAn workflow. The quickest way to get started is to download the virtual machine or use an AWS instance.</p>
</div>
<div id="demo-table" class="section level3">
<h3><span class="header-section-number">5.0.1</span> PEcAn Demos</h3>
<p>The following Tutorials assume you have installed PEcAn. If you have not, please consult the <a href="pecan-manual-setup.html#pecan-manual-setup">PEcAn Installation Section</a>.</p>
<table>
<thead>
<tr class="header">
<th align="center">Type</th>
<th align="center">Title</th>
<th align="center">Web Link</th>
<th align="center">Source Rmd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Demo</td>
<td align="center">Basic Run</td>
<td align="center"><a href="https://pecanproject.github.io/pecan-documentation/tutorials/Demo01.html">html</a></td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/develop/documentation/tutorials/01_Demo_Basic_Run/Demo01.Rmd">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Demo</td>
<td align="center">Uncertainty Analysis</td>
<td align="center"><a href="https://pecanproject.github.io/pecan-documentation/tutorials/Demo02.html">html</a></td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/master/documentation/tutorials/02_Demo_Uncertainty_Analysis">Rmd</a></td>
</tr>
<tr class="odd">
<td align="center">Demo</td>
<td align="center">Output Analysis</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/develop/documentation/tutorials/AnalyzeOutput">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Demo</td>
<td align="center">MCMC</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/develop/documentation/tutorials/MCMC">Rmd</a></td>
</tr>
<tr class="odd">
<td align="center">Demo</td>
<td align="center">Parameter Assimilation</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/develop/documentation/tutorials/ParameterAssimilation">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Demo</td>
<td align="center">State Assimilation</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/develop/documentation/tutorials/StateAssimilation">Rmd</a></td>
</tr>
<tr class="odd">
<td align="center">Demo</td>
<td align="center">Sensitivity</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/develop/documentation/tutorials/sensitivity">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Vignette</td>
<td align="center">Allometries</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/master/modules/allometry/vignettes/AllomVignette.Rmd">Rmd</a></td>
</tr>
<tr class="odd">
<td align="center">Vignette</td>
<td align="center">MCMC</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/master/modules/assim.batch/vignettes/AssimBatchVignette.Rmd">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Vignette</td>
<td align="center">Meteorological Data</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/tree/master/modules/data.atmosphere/vignettes">Rmd</a></td>
</tr>
<tr class="odd">
<td align="center">Vignette</td>
<td align="center">Meta-Analysis</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/master/modules/meta.analysis/vignettes/single.MA_demo.Rmd">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Vignette</td>
<td align="center">Photosynthetic Response Curves</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/master/modules/photosynthesis/vignettes/ResponseCurves.Rmd">Rmd</a></td>
</tr>
<tr class="odd">
<td align="center">Vignette</td>
<td align="center">Priors</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/master/modules/priors/vignettes/priors_demo.Rmd">Rmd</a></td>
</tr>
<tr class="even">
<td align="center">Vignette</td>
<td align="center">Leaf Spectra:PROSPECT inversion</td>
<td align="center">html</td>
<td align="center"><a href="https://github.com/PecanProject/pecan/blob/master/modules/rtm/vignettes/pecanrtm.vignette.Rmd">Rmd</a></td>
</tr>
</tbody>
</table>

</div>
<div id="basic-web-wrokflow" class="section level2">
<h2><span class="header-section-number">5.1</span> Basic Web workflow</h2>
<p>This chapter describes the major steps of the PEcAn web-based workflow, which are as follows:</p>
<ul>
<li><a href="user-section.html#web-site-model">Model and site selection</a></li>
<li><a href="user-section.html#web-model-config">Model configuration</a></li>
<li>Run execution – TODO!</li>
<li>Results – TODO!</li>
<li>Interactive visualizations – TODO!</li>
</ul>
<p>We recommend that all new users begin with [PEcAn Hands-On Demo 01: Basic Run]. The documentation below assumes you are already familiar with how to navigate to PEcAn’s interactive web interface for running models.</p>
<div id="web-site-model" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Site and model selection</h3>
<p>This page is used to select the model to run and the site at which you would like to run that model.</p>
<p><strong>NOTE:</strong> If this page does not load for you, it may be related to a known Google Maps API key issue. See <a href="https://github.com/PecanProject/pecan/issues/1269">issue #1269</a> for a possible solution.</p>
<div id="selecting-a-model" class="section level4">
<h4><span class="header-section-number">5.1.1.1</span> Selecting a model</h4>
<ol style="list-style-type: decimal">
<li><p>On the <strong>Select Host</strong> webpage <strong>use the Host pull-down menu to select the server you want to run on</strong>. PEcAn is designed to allow models to be run both locally and on remote high-performance computing (HPC) resources (i.e. clusters). We recommend that users start with local runs. More information about connecting your PEcAn instance to a cluster can be found on the <a href="user-section.html#pecan-remote">Remote execution with PEcAn</a> page.</p></li>
<li><p>Next, <strong>select the model you want to run under the Model pull-down menu</strong>. The list of models currently supported by PEcAn, along with information about these models, is available on the <a href="pecan-models.html#pecan-models">PEcAn Models</a> page.</p>
<ol style="list-style-type: lower-roman">
<li><p>If a PEcAn-supported model is not listed, this is most likely because the model has not been installed on the server. The PEcAn team does not have permissions to redistribute all of the models that are coupled to it, so you will have to install some PEcAn-compatible models yourself. Please consult the PEcAn model listing for information about obtaining and installing different models. Once the model is installed and you have added the location of the model executable to Bety (see <a href="user-section.html#adding-model">Adding An Ecosystem Model</a>), your model should appear on the PEcAn <strong>Select Host</strong> page after your refresh the page.</p></li>
<li><p>If you would like to add a new model to PEcAn please consult our guide for <a href="user-section.html#adding-model">Adding an Ecosystem Model</a> and contact the PEcAn team for assistance.</p></li>
</ol></li>
<li><p>If selecting your model causes your <strong>site to disappear</strong> from the Google Map, that means the site exists but there are no drivers for that model-site combination registered in the database.</p>
<ol style="list-style-type: lower-roman">
<li><p>Click the “Conversion” checkbox. If your site reappears, that means PEcAn should be able to automatically generate the required inputs for this site by converting from existing input files in other formats.</p></li>
<li><p>If the site still does not reappear, that means there are required input files for that model-site combination that PEcAn cannot autogenerate. This may be because the model has unique input requirements or because it has not yet been fully coupled to the PEcAn input processing workflow. Go to the troubleshooting section under <a href="user-section.html#selecting-a-site">Selecting a site</a> for more information on diagnosing what drivers are missing.</p></li>
</ol></li>
</ol>
</div>
<div id="selecting-a-site" class="section level4">
<h4><span class="header-section-number">5.1.1.2</span> Selecting a site</h4>
</div>
<div id="site-groups" class="section level4">
<h4><span class="header-section-number">5.1.1.3</span> Site Groups</h4>
<ol style="list-style-type: decimal">
<li><p>PEcAn provides the option of organizing sites into groups to make them easier to find and easier to run as a group. We have pre-loaded a number of common research networks (e.g., FLUXNET, LTER, NEON), but you are free to create new site groups through Bety.</p></li>
<li><p>If you are searching for a site that is not part of an existing site group, or you are unsure which site group it belongs to, select “All Sites” to see all sites in Bety. Note that this may take a while to render.</p></li>
</ol>
</div>
<div id="using-existing-sites" class="section level4">
<h4><span class="header-section-number">5.1.1.4</span> Using existing sites</h4>
<ol style="list-style-type: decimal">
<li><p><strong>Find the site on the map</strong> The simplest way of determining if a site exists in PEcAn is through the Google Map interface of the web-based workflow. You’ll want to make sure that the “Site Group” is set to “All Sites” and the “Model” is set to “All Models”.</p></li>
<li><p><strong>Find the site in BETY</strong> If the site is not on the map, it may still be in Bety but with insufficient geographic information. To locate the site in Bety, first login to your local version of the BETY database. If using the VM, navigate to <code>localhost:6480/bety</code> and login with username <code>bety</code> and password <code>illinois</code>. Then, navigate to <code>Data &gt; Sites</code> and use the “Search” box to search for your site. If you <strong>do</strong> find your site, click “Edit” and add geographic information so that the site will show up on the map. Also, note that the site ID number shows up in the URL for the “Show” or “Edit” pages. This ID is often useful to know, for example when editing a PEcAn settings file by hand. If you did not find you site, follow the instructions below to add a site.</p></li>
</ol>
</div>
<div id="adding-a-new-site" class="section level4">
<h4><span class="header-section-number">5.1.1.5</span> Adding a new site</h4>
<p>(TODO: Move most of this out)</p>
<ol style="list-style-type: decimal">
<li><p>Log into Bety as described above.</p></li>
<li><p><strong>Pick a citation for your site</strong> Each site requires an associated “citation” that must be added before the site itself is added. First, navigate to “Data &gt; Citations” and use the “Search” box to see if the relevant citation already exists. If it does, click the check mark under “Actions” to proceed to site creation.</p></li>
</ol>
<ul>
<li><p><strong>To create a new citation</strong>, click the <strong>New Citation</strong> button, fill in the fields, and then click “Create”. The “field URL” should contain the web address that takes you to this publication on the publisher’s website. The “PDF” field should be the full web address to a PDF for this citation.</p></li>
<li><p>Note that our definition of a citation is flexible, and a citation need not be a peer-reviewed publication. Most of the fields in “New Citation” can be left blank, but we recommend at least adding a descriptive title, such as “EBI Farm Field Data” and a relevant contact person as the “Author”.</p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><p>Once the Citation is created or selected this should automatically take you to the Sites page and list any Sites already associated with this citation. To create a new site click the <strong>New Site</strong> button.</p></li>
<li><p>When creating a new site, the most important fields are the <strong>Site name</strong> and coordinates (<strong>latitude</strong> and <strong>longitude</strong>). The coordinates can be entered by hand or by clicking on the site location on the Google Map interface. All other information is optional, but can be useful for searching and indexing purposes.</p></li>
<li><p>When you are done click <strong>Create</strong>. At this point, once the PEcAn site-level page is refreshed, the site should automatically appear.</p></li>
</ol>
</div>
<div id="troubleshooting-2" class="section level4">
<h4><span class="header-section-number">5.1.1.6</span> Troubleshooting</h4>
<div id="my-site-shows-up-when-i-dont-have-any-model-selected-but-disappears-once-i-select-the-model-i-want-to-run" class="section level5">
<h5><span class="header-section-number">5.1.1.6.1</span> My site shows up when I don’t have any model selected, but disappears once I select the model I want to run</h5>
<p>Selecting a model will cause PEcAn to filter the available sites based on whether they possess the required Inputs for a given model (e.g. meteorology). To check what Inputs are missing for a site point your browser to the pecan/checksite.php webpage (e.g. localhost:6480/pecan/checksite.php). This page looks virtually identical to the site selection page, except that it has a <em>Check</em> button instead of <em>Prev</em> and <em>Next</em>. If you select a Machine, Model, and Site and then click <em>Check</em> the page should return a list of what Inputs are missing (listing both the name and the Format ID number). Don’t forget that its possible for PEcAn to have required Inputs in its database, but just not have them for the Machine where you want to run.</p>
<p>To see more about what Inputs a given model can accept, and which of those are required, take a look at the MODEL_TYPE table entry in the database (e.g. go to <code>localhost:6480/bety</code>; Select <code>Runs &gt; Model Type</code>; and then click on the model you want to run).</p>
<p>For information about loading missing Inputs into the database visit <a href="user-section.html#input-records-in-bety">Input records in BETY</a>, and also read the rest of the pages under this section, which will provide important information about the specific classes of Inputs (e.g. meteorology, vegetation, etc).</p>
<p>Finally, we are continually developing and refining workflows and standards for processing Input data in a model-agnostic way. The details about what Inputs can be processed automatically are discussed input-by-input in the sections below. For those looking to dive into the code or troubleshoot further, these conversions are ultimately handled under the <code>PEcAn.workflow::do_conversions</code> workflow module.</p>
</div>
</div>
<div id="web-model-config" class="section level4">
<h4><span class="header-section-number">5.1.1.7</span> Model configuration</h4>
<p>This page is used for basic model configuration, including when your model will run and what input data it will use.</p>
</div>
<div id="choosing-meteorology" class="section level4">
<h4><span class="header-section-number">5.1.1.8</span> Choosing meteorology</h4>
<p>Once a Machine, Model, and Site have been selected, PEcAn will take you to the Input selection page. From this page you will select what Plant Functional Type (PFT) you want to run at a site, the start and end dates of the run, and various Input selections. The most common of these across all models is the need to specify meteorological forcing data. The exact name of the menu item for meteorology will vary by model because all of the Input requirements are generated individually for each model based on the MODEL_TYPE table. In general there are 3 possible cases for meteorology</p>
<ul>
<li>PEcAn already has driver files in its database</li>
<li>PEcAn does not have drivers, but can generate them from publicly available data</li>
<li>You need (or want) to upload your own drivers</li>
</ul>
<p>The first two cases will appear automatically in the the pull down menu. For meteorological files that already exist you will see the date range that’s available. By contrast, met that can be generated will appear as “Use <source>”, where <source> is the origin of the data (e.g. “Use Ameriflux” will use the micromet from an Ameriflux eddy covariance tower, if one is present at the site).</p>
<p>If you want to upload your own met data this can be done in three ways.</p>
<ol style="list-style-type: decimal">
<li><p>The default way to add met data is to incorporate it into the overall meteorological processing workflow. This is preferred if you are working with a common meteorological data product that is not yet in PEcAn’s workflow. This case can be divided into two special cases:</p>
<ol style="list-style-type: lower-roman">
<li><p>Data is in a common MIME-type that PEcAn already has a converter for (e.g. CSV). In this case you’ll want to create a new Format record for the meta-data so that the existing converter can process this data. See documentation for [Creating a new Format record in BETY] for more details.</p></li>
<li><p>Data is in a more complicated format or interactive database, but large/useful enough to warrent a custom conversion function. Details on creating custom met conversions is in the [Input Conversion], though at this stage you would also be strongly encouraged to contact the PEcAn development team.</p></li>
</ol></li>
<li><p>The second-best way is to upload data in PEcAn’s standard meteorological format (netCDF files, CF metadata). See [Input Conversion] for details about variables and units. From this standard, PEcAn can then convert the file to the model-specific format required by the model you have chosen. This approach is preferred for a rare or one-off meterological file format, because PEcAn will also be able to convert the file into the format required by any other model as well.</p></li>
<li><p>The last option for adding met data is to add it in a model-specific format, which is often easiest if you’ve already been running your model at a site and are just switching to using PEcAn.</p></li>
</ol>
</div>
<div id="met-workflow" class="section level4">
<h4><span class="header-section-number">5.1.1.9</span> Met workflow</h4>
<p>In a nutshell, the PEcAn met workflow is designed to reduce the problem of converting <em>n</em> possible met inputs into <em>m</em> possible model formats, which requires <em>n x m</em> conversion functions as well as numerous custom functions for downscaling, gap filling, etc. Instead, PEcAn works with a single met standard, and thus requires <em>n</em> conversion functions, one for converting each data source into the PEcAn standard, and then <em>m</em> conversion functions for converting from that standard to what an individual model requires. For a new model joining the PEcAn system the burden in particularly low – writing one conversion function provides access to <em>n</em> inputs. Similarly, PEcAn performs all other operations/manipulations (extracting a site, downscaling, gap filling, etc) within the PEcAn standard, which means these operations only need be implemented once.</p>
<p>Consider a generic met data product named MET for simplicity. PEcAn will use a function, download.MET, to pull data for the selected year from a public data source (e.g. Ameriflux, North American Regional Reanalysis, etc). Next, PEcAn will use a function, met2CF.MET, to convert the data into the PEcAn standard. If the data is already at the site scale it will then gapfill the data. If the data is a regional or global data product, PEcAn will then permute the data to allow easier site-level extraction, then it will extract data for the requested site and data range. Modules to address the temporal and spatial downscaling of meteorological data products, as well as their uncertainties, are in development but not yet part of the operational workflow. All of these functions are located within the data.atmosphere module.</p>
<p>Once data is in the standard format and processed, it will be converted to the model-specific format using a met2model.MODEL function (located in that MODEL’s module).</p>
<p>More detailed information on how PEcAn processes inputs can be found on our [Input Conversion] page.</p>
</div>
<div id="troubleshooting-meteorological-conversions" class="section level4">
<h4><span class="header-section-number">5.1.1.10</span> Troubleshooting meteorological conversions</h4>
<p>At the current moment, most of the issues below address possible errors that the Ameriflux meteorology workflow might report</p>
<div id="could-not-do-gapfill-the-following-variables-have-nas" class="section level5">
<h5><span class="header-section-number">5.1.1.10.1</span> Could not do gapfill … The following variables have NA’s</h5>
<p>This error message means that there were gaps in the downloaded data, for whatever variables that were listed, which were larger than the current algorithm could fill. Particularly common is missing radiation or PAR data, as Ameriflux frequently converts nighttime data to NULL, and work is in progress to detect this based on solar geometry. Also common are incomplete years (first or last year of tower operations).</p>
</div>
<div id="could-not-get-information-about-.-is-this-an-ameriflux-site" class="section level5">
<h5><span class="header-section-number">5.1.1.10.2</span> Could not get information about <site> . Is this an Ameriflux site?</h5>
<p>This message occurs when PEcAn believes that a site is part of Ameriflux (because it was listed on the Ameriflux or FLUXNET webpage and has a US-* site code), but no data is present on the Ameriflux server. The most common reasons for this is that you have selected a site that has not submitted data to Ameriflux yet (or that data hasn’t been processed yet), or you have selected a year that’s outside the tower’s operational period. Visit <a href="http://ameriflux.lbl.gov/sites/site-list-and-pages/">Ameriflux</a> and <a href="http://fluxnet.ornl.gov/site_status">FLUXNET</a> for lists of available site years.</p>
</div>
<div id="could-not-download-data-for-for-the-year" class="section level5">
<h5><span class="header-section-number">5.1.1.10.3</span> Could not download data for <site> for the year <YEAR></h5>
<p>This is similar to the previous error, but in this case PEcAn did find data for the site listed, but just not for the year requested. This can usually be fixed by just altering the years of the run to match those with available data.</p>
</div>
<div id="i-could-not-find-the-requested-var-or-dimvar-in-the-file" class="section level5">
<h5><span class="header-section-number">5.1.1.10.4</span> I could not find the requested var (or dimvar) in the file!</h5>
<p>PEcAn could not find a required variable within the downloaded file. Most likely this is due to that variable not being measured at this site. The most common cause of failure is the absence of atmospheric pressure data (PRESS), but since most models have a low sensitivity to this variable we are working on methods to estimate this from other sources.</p>
</div>
</div>
<div id="selecting-plant-functional-types-pfts-and-other-parameter-groupings." class="section level4">
<h4><span class="header-section-number">5.1.1.11</span> Selecting Plant Functional Types (PFTs) and other parameter groupings.</h4>
</div>
<div id="using-existing-pfts" class="section level4">
<h4><span class="header-section-number">5.1.1.12</span> Using existing PFTs</h4>
<p>PEcAn does not automatically know what vegetation types are present at your study site so you need to select the PFT.</p>
<p>Some models, such as ED2 and LINKAGES, support competition among multiple PFTs and thus you are encouraged to highlight multiple choices. Other models, such as SIPNET and DALEC, only support one PFT at a site.</p>
<p>Many models also have parameters that control non-vegetation processes (e.g. soil biogeochemistry and hydrology). PEcAn allows users to assign these parameters to functional groups as well (e.g. a <code>soils</code> PFT)</p>
</div>
<div id="creating-new-pfts" class="section level4">
<h4><span class="header-section-number">5.1.1.13</span> Creating new PFTs</h4>
<p>To modify or add a new Plant Functional Type (PFT), or to change a PFT’s priors, navigate
on the grey menu bar to Data &gt; PFTs</p>
<ol style="list-style-type: decimal">
<li><p>To add a new pft, click “new PFT” at the top and enter a name and description. (hint:
we’re trying to name PFTs based on model.biome.pft, ED2 is the default model if one
isn’t specified)</p></li>
<li><p>To add new species to a PFT click on [+] View Related Species and type the species,
genus, or family you are looking for into the Search box. Click on the + to add.</p></li>
<li><p>To remove a species from a PFT, click on [+] View Related Species and click on the X
of the species you want to remove from the PFT.</p></li>
<li><p>To remove a prior, click [-] View Related Prior and click on the X of the variable who’s
prior you want to remove. This will cause the parameter to be excluded from all
analyses (meta-analysis, sensitivity analysis, etc) and revert to its default value.</p></li>
<li><p>To add a prior, choose one from the white box of priors on the right to choose.</p></li>
<li><p>To view the specification of a prior, or to add a new prior, click BETY-DB &gt; Priors and
enter the information on the variable, distribution name, distribution parameters, etc. N
is the sample size underlying the prior specification (0 is ok for uninformative priors).</p></li>
<li><p>You can also got to Data &gt; Variables in order to use the search function to find an
existing variable (or create a new one). Please try not to create new variables
unnecessarily (e.g. changes of variable name or units to what your model uses is handled
internally, so you want to find the trait with the correct MEANING).</p></li>
</ol>
<p>Additional information on adding PFTs, Species, and Priors can be found in Adding An [Ecosystem Model].</p>
</div>
<div id="choosing-initial-vegetation" class="section level4">
<h4><span class="header-section-number">5.1.1.14</span> Choosing initial vegetation</h4>
<p>On the Input Selection webpage, in addition to selecting PFTs, start &amp; end dates, and meteorology, many models also require some way of specifying the initial conditions for the vegetation, which may range from setting the aboveground biomass and LAI up to detailed inventory-like data on species composition and stand structure.</p>
<p>At the moment, PEcAn has three cases for initial conditions:</p>
<ol style="list-style-type: decimal">
<li><p>If files already exist in the database, they can simply be selected from the menu. For ED2, there are 3 different veg files (site, pss, css) and it is important that you select a complete set, not mix and match.</p></li>
<li><p>If files don’t exist they can be uploaded following the instructions in <a href="user-section.html#create-a-database-file-record-for-the-input-data">Create a database file record for the input data</a>.</p></li>
<li><p>Automated vegetation initial condition workflow</p></li>
</ol>
<p>As with meteorology, PEcAn is working to develop a model-agnostic workflow for converting various sources of vegetation data to common standards, developing common processing tools, and then writing out to model-specific formats. This process is in a much early stage than the meteorology workflow, as we are still researching what the options are for standard formats, but ultimately aims to be much more broad in scope, considering not just plot inventory data but also historical documentation, paleoecological proxies, satellite remote sensing (e.g. LANDSAT), airborne hyperspectral imagery, and active remote sensing (Lidar, Radar).</p>
<p>At the moment, what is functional is a prototype workflow that works for inventory-based vegetation data. This data can come from either files that have been registered with the BETY Inputs and Formats tables or can be queried from the USFS Forest Inventory and Analysis (FIA). For more information visit Section 13.1.2.2 Vegetation Data</p>
</div>
<div id="us-fia" class="section level4">
<h4><span class="header-section-number">5.1.1.15</span> US FIA</h4>
<p>This tool works with an internal copy of the FIA that is uploaded to a postGRES database along side BETY, however for space reasons this database does not ship with the PEcAn VM. To turn this feature on:</p>
<ol style="list-style-type: decimal">
<li>Download and Install the FIA database. Instructions in <a href="pecan-manual-setup.html#install-data">Installing data for PEcAn</a></li>
<li>For web-base runs, specify the database settings in the <a href="https://github.com/PecanProject/pecan/blob/master/web/config.example.php">config.php</a></li>
<li>For R-based runs, specify the database settings in the <a href="pecanXML.html#pecanXML">THE PEcAn XML</a></li>
</ol>
<p>More detailed information on how PEcAn processes inputs can be found on our [Input Conversion]page.</p>
</div>
<div id="spin-up" class="section level4">
<h4><span class="header-section-number">5.1.1.16</span> Spin up</h4>
<p>A number of ecosystem models are typically initialized by spinning up to steady state. At the moment PEcAn doesn’t handle spin up automatically (e.g. looping met, checking for stability), but there are various ways to achieve a spin-up within the system.</p>
<p><strong>Option 1:</strong> If there are model-specific settings in a model’s settings/config file, then that file can be accessed by clicking on the <strong>Edit model config</strong> check box. If this box is selected then PEcAn will pause the site run workflow after it has generated your model config file, but before it runs the model, and give you an opportunity to edit the file by hand, allowing you to change any model-specific spin up settings (e.g met recycling, spin up length)</p>
<p><strong>Option 2:</strong> Set start_year very early and set the met drivers to be a long time series (e.g. PalEON, something custom uploaded to Inputs)</p>
<p><strong>Option 3:</strong> In the MODEL_TYPE table, add your model’s restart format as an optional input, modify the model specific write.config function to use that restart, and then load a previous spin-up to the Inputs table</p>
<p>Beyond these options, we hope to eventually develop more general, model-agnostic tools for spin up. In particular, we have started to explore the accelerated spin-up and semi-analytical techniques being developed by Yiqi Luo’s lab</p>
</div>
<div id="selecting-a-soils-product" class="section level4">
<h4><span class="header-section-number">5.1.1.17</span> Selecting a soils product</h4>
<p>Many models have requirements for soils information, which may include: site-specific soil texture and depth information; soil biogeochemical initial conditions (e.g. soil carbon and nitrogen pools); soil moisture initial conditions; and soil thermal initial conditions.</p>
<p>As with <a href="user-section.html#choosing-initial-vegetation">Choosing initial vegetation</a>, we eventually hope to develop data standards, soils workflows, and spin-up tools, but at the moment this workflow is in the early stages of development. Model requirements need to be met by<a href="user-section.html#creating-a-new-input-record-in-bety">Creating a new Input record in BETY</a> into the database or using files that have already been uploaded. Similar to met, we recommend that this file be in the PEcAn-standard netCDF described below, but model-specific files can also be registered.</p>
</div>
<div id="soil-texture-depth-and-physical-parameters" class="section level4">
<h4><span class="header-section-number">5.1.1.18</span> Soil texture, depth, and physical parameters</h4>
<p>A PEcAn-standard netCDF file format exists for soil texture, depth, and physical parameters, using PEcAn standard names that are largely a direct extention of the CF standard.</p>
<p>The easiest way to create this file is with the PEcAn R function <code>soil2netcdf</code> as described in the Soil Data section of the Advanced Users Guide.</p>
<p>A table of standard names and units can be listed using <code>PEcAn.data.land::soil.units()</code> with no arguments.</p>
<p>More detailed information on how PEcAn processes inputs can be found on our [Input Conversion] page.</p>
</div>
<div id="other-model-inputs" class="section level4">
<h4><span class="header-section-number">5.1.1.19</span> Other model inputs</h4>
<p>Finally, any other model-specific inputs (e.g. N deposition, land use history, etc), should be met by <a href="user-section.html#creating-a-new-input-record-in-bety">Creating a new Input record in BETY</a> or using files that have already been uploaded.</p>

</div>
</div>
</div>
<div id="intermediate-user" class="section level2">
<h2><span class="header-section-number">5.2</span> PEcAn Web Interface</h2>
<p>This section will provide information to those wanting to take advantage of PEcAn’s customizations from the web interface.</p>
<ul>
<li><a href="user-section.html#additional-web-configuration">Additional web configuration</a> - Advanced options available from the web interface
<ul>
<li><a href="user-section.html#browndog">Brown Dog</a></li>
<li><a href="user-section.html#advanced-setup">Sensitivity and ensemble analyses</a>[TODO: Under construction…]</li>
<li>[Editing model configurations][TODO: Under construction…]</li>
</ul></li>
<li><a href="user-section.html#settings-configured-analyses">Settings-configured analyses</a> - Analyses only available by manually editing <code>pecan.xml</code>
<ul>
<li><a href="user-section.html#pda">Parameter data assimilation (PDA)</a></li>
<li><a href="user-section.html#sda">State data assimilation (SDA)</a></li>
</ul></li>
<li><a href="user-section.html#pecan-remote">Remote execution with PEcAn</a> - Running analyses and generally working with external machines (HPC) in the context of PEcAn.</li>
</ul>

<div id="additional-web-configuration" class="section level4">
<h4><span class="header-section-number">5.2.0.1</span> Additional web configuration</h4>
<p>Additional settings for web configuration:</p>
<ul>
<li><a href="user-section.html#browndog">Brown Dog</a></li>
<li>[Editing model configuration files]</li>
<li><a href="user-section.html#advanced-setup">Advanced setup</a>
<ul>
<li>[Sensitivity analysis]</li>
<li>[Uncertainty analysis]</li>
</ul></li>
</ul>
</div>
<div id="browndog" class="section level4">
<h4><span class="header-section-number">5.2.0.2</span> Brown Dog</h4>
<p>The Browndog service provides PEcAn with access to large and diverse sets of data at the click of a button in the format that PEcAn needs. By clicking the checkbox you will be using the Browndog Service to process data.</p>
<p>For more information regarding meteorological data check out <a href="available-meteorological-drivers.html#available-meteorological-drivers">Available Meteorological Drivers</a></p>
<p>** <a href="http://browndog.ncsa.illinois.edu/">More Informatoin can be found at the Browndog website</a>**</p>
</div>
<div id="basic-setups" class="section level4">
<h4><span class="header-section-number">5.2.0.3</span> Basic Setups</h4>
<p>There are few options which you can change via web interface.</p>
<p>To visit the configuration page either you can just click on the setups link on the introduction page alternatively can type <code>&lt;host&gt;/setups/</code>.</p>
<p>The list of configuration available</p>
<ol style="list-style-type: decimal">
<li><p><strong>Database configuration</strong> : BETYdb(Biofuel Ecophysiological Traits and Yields database) configuration details, can be edited according to need.</p></li>
<li><p><strong>Browndog configuration</strong> : Browndog configuration details, Used to connect browndog. Its included by default in VM.</p></li>
<li><p><strong>FIA Database</strong> : FIA(Forest Inventory and Analysis) Database configuration details, Can be used to add additional data to models.</p></li>
<li><p><strong>Google MapKey</strong> : Google Map key, used to access the google map by PEcAn.</p></li>
<li><p><strong>Change Password</strong> : A small infomation to change the VM user password. (if using Docker image it won’t work)</p></li>
<li><p><strong>Automatic Sync</strong> : If ON then it will sync the database between local machine and the remote servers. <strong>Still unders testing part might be buggy</strong>.</p></li>
</ol>
<p>Still work on the adding other editing feature going on, this page will be updated as new configuration will be available.</p>
</div>
<div id="advanced-setup" class="section level4">
<h4><span class="header-section-number">5.2.0.4</span> Advanced Setup</h4>
<p>(TODO: Under construction…)</p>
</div>
<div id="editing-model-configurations" class="section level4">
<h4><span class="header-section-number">5.2.0.5</span> Editing model configurations</h4>
<p>(TODO: Under construction…)</p>

</div>
<div id="settings-configured-analyses" class="section level4">
<h4><span class="header-section-number">5.2.0.6</span> Settings-configured analyses</h4>
<p>These analyses can be run through the web interface, but lack graphical interfaces and currently can only be configured throughthe XML settings. To run these analyses use the <strong>Edit pecan.xml</strong> checkbox on the Input configuration page. Eventually, these modules will be integrated into the web user interface.</p>
<ul>
<li><a href="user-section.html#pda">Parameter Data Assimilation (PDA)</a></li>
<li><a href="user-section.html#sda">State Data Assimilation (SDA)</a></li>
<li><a href="user-section.html#multisettings">MultiSettings</a></li>
<li><a href="user-section.html#benchmarking">Benchmarking</a></li>
</ul>
<p>(TODO: Add links)</p>
</div>
<div id="pda" class="section level4">
<h4><span class="header-section-number">5.2.0.7</span> Parameter data assimilation (PDA)</h4>
<p>All functions pertaining to Parameter Data Assimilation are housed within: <strong>pecan/modules/assim.batch</strong>.</p>
<p>For a detailed usage of the module, please see the vignette under <strong>pecan/modules/assim.batch/vignettes</strong>.</p>
</div>
<div id="pda.mcmc.r" class="section level4">
<h4><span class="header-section-number">5.2.0.8</span> <strong>pda.mcmc.R</strong></h4>
<p>This is the main PDA code. It performs Bayesian MCMC on model parameters by proposing parameter values, running the model, calculating a likelihood (between model output and supplied observations), and accepting or rejecting the proposed parameters (Metropolis algorithm). Additional notes:</p>
<ul>
<li><p>The first argument is <em>settings</em>, followed by others that all default to <em>NULL.settings</em> is a list used throughout Pecan, which contains all the user options for whatever analyses are being done. The easiest thing to do is just pass that whole object all around the Pecan code and let different functions access whichever settings they need. That’s what a lot of the rest of the Pecan code does. But the flexibility to override most of the relevant settings in <em>settings</em> is there by providing them directly as arguments to the function.</p></li>
<li><p>The <em>if(FALSE)…</em> : If you’re trying to step through the function you probably will have the <em>settings</em> object around, but those other variables will be undefined. If you set them all to NULL then they’ll be ignored without causing errors. It is there for debugging purposes.</p></li>
<li><p>The next step calls pda.settings(), which is in the file pda.utils.R (see below). It checks whether any settings are being overridden by arguments, and in most cases supplies default values if it can’t find either.</p></li>
<li>In the MCMC setup section
<ul>
<li>The code is set up to allow you to start a new MCMC chain, or to continue a previous chain as specified in settings.</li>
<li>The code writes a simple text file of parameter samples at every iteration, which lets you get some results and even re-start an MCMC that fails for some reason.</li>
<li>The code has adaptive jump distributions. So you can see some initialization of the jump distributions and associated variables here.</li>
<li>Finally, note that after all this setup a new XML settings file is saved. The idea is that the original pecan.xml you create is preserved for provenance, and then periodically throughout the workflow the settings (likely containing new information) are re-saved with descriptive filenames.</li>
</ul></li>
<li>MCMC loop
<ul>
<li>Periodically adjust jump distribution to make acceptance rate closer to target</li>
<li>Propose new parameters one at a time. For each:
<ul>
<li>First, note that Pecan may be handling many more parameters than are actually being targeted by PDA. Pecan puts priors on any variables it has information for (in the BETY database), and then these get passed around throughout the analysis and every step (meta-, sensitivity, ensemble analyses, etc.). But for PDA, you specify a separate list of probably far fewer parameters to constrain with data. These are the ones that get looped over and varied here. The distinction between all parameters and only those dealt with in PDA is dealt with in the setup code above.</li>
<li>First a new value is proposed for the parameter of interest.</li>
<li>Then, a new model run is set up, identical to the previous except with the new proposed value for the one parameter being updated on this run.</li>
<li>The model run is started, and outputs collected after waiting for it to finish.</li>
<li>A new likelihood is calculated based on the model outputs and the observed dataset provided.</li>
<li>Standard Metropolis acceptance criteria is used to decide whether to keep the proposed parameter.</li>
<li>Periodically (at interval specified in settings), a diagnostic figure is saved to disk so you can check on progress.</li>
</ul></li>
<li>This works only for NEE currently</li>
</ul></li>
</ul>
</div>
<div id="pda.mcmc.bs.r" class="section level4">
<h4><span class="header-section-number">5.2.0.9</span> <strong>pda.mcmc.bs.R</strong></h4>
<p>This file is basically identical to pda.mcm.R, but rather than propose parameters one at a time, it proposes new values for all parameters at once (“bs” stands for “block sampling”). You choose which option to use by specifying settings<span class="math inline">\(assim.batch\)</span>method:
* “bruteforce” means sample parameters one at a time
* “bruteforce.bs” means use this version, sampling all parameters at once
* “emulator” means use the emulated-likelihood version</p>
</div>
<div id="pda.emulator" class="section level4">
<h4><span class="header-section-number">5.2.0.10</span> <strong>pda.emulator</strong></h4>
<p>This version of the PDA code again looks quite similar to the basic “bruteforce” one, but its mechanics are very different. The basic idea is, rather than running thousands of model iterations to explore parameter space via MCMC, run a relatively smaller number of runs that have been carefully chosen to give good coverage of parameter space. Then, basically interpolate the likelihood calculated for each of those runs (actually, fit a Gaussian process to it), to get a surface that “emulates” the true likelihood. Now, perform regular MCMC (just like the “bruteforce” approach), except instead of actually running the model on every iteration to get a likelihood, just get an approximation from the likelihood emulator. Since the latter step takes virtually no time, you can run as long of an MCMC as you need at little computational cost, once you have done the initial model runs to create the likelihood emulator.</p>
</div>
<div id="pda.mcmc.recover.r" class="section level4">
<h4><span class="header-section-number">5.2.0.11</span> <strong>pda.mcmc.recover.R</strong></h4>
<p>This function is for recovering a failed PDA MCMC run.</p>
</div>
<div id="pda.utils.r" class="section level4">
<h4><span class="header-section-number">5.2.0.12</span> <strong>pda.utils.R</strong></h4>
<p>This file contains most of the individual functions used by the main PDA functions (pda.mcmc.*.R).</p>
<ul>
<li><em>assim.batch</em> is the main function Pecan calls to do PDA. It checks which method is requested (bruteforce, bruteforce.bs, or emulator) and call the appropriate function described above.</li>
<li><em>pda.setting</em> handles settings. If a setting isn’t found, the code can usually supply a reasonable default.</li>
<li><em>pda.load.priors</em> is fairly self explanatory, except that it handles a lot of cases and gives different options priority over others. Basically, the priors to use for PDA parameters can come from either a Pecan prior.distns or post.distns object (the latter would be, e.g., the posteriors of a meta-analysis or previous PDA), or specified either by file path or BETY ID. If not told otherwise, the code tries to just find the most recent posterior in BETY, and use that as prior for PDA.</li>
<li><em>pda.create.ensemble</em> gets an ensemble ID for the PDA. All model runs associated with an individual PDA (any of the three methods) are considered part of a single ensemble. This function does is register a new ensemble in BETY, and return the ID that BETY gives it.</li>
<li><em>pda.define.prior.fn</em> creates R functions for all of the priors the PDA will use.</li>
<li><em>pda.init.params</em> sets up the parameter matrix for the run, which has one row per iteration, and one column per parameter. Columns include all Pecan parameters, not just the (probably small) subset that are being updated by PDA. This is for compatibility with other Pecan components. If starting a fresh run, the returned matrix is just a big empty matrix to fill in as the PDA runs. If continuing an existing MCMC, then it will be the previous params matrix, with a bunch of blank rows added on for filling in during this round of PDA.</li>
<li><em>pda.init.run</em> This is basically a big wrapper for Pecan’s write.config function (actually functions [plural], since every model in Pecan has its own version). For the bruteforce and bruteforce.bs methods this will be run once per iteration, whereas the emulator method knows about all its runs ahead of time and this will be a big batch of all runs at once.</li>
<li><em>pda.adjust.jumps</em> tweaks the jump distributions for the standard MCMC method, and <em>pda.adjust.jumps.bs</em> does the same for the block-sampled version.</li>
<li><em>pda.calc.llik</em> calculates the log-likelihood of the model given all datasets provided to compare it to.</li>
<li><em>pda.generate.knots</em> is for the emulator version of PDA. It uses a Latin hypercube design to sample a specified number of locations in parameter space. These locations are where the model will actually be run, and then the GP interpolates the likelihood surface in between.</li>
<li><em>pda.plot.params</em> provides basic MCMC diagnostics (trace and density) for parameters being sampled.</li>
<li><em>pda.postprocess</em> prepares the posteriors of the PDA, stores them to files and the database, and performs some other cleanup functions.</li>
<li><em>pda.load.data.r</em> This is the function that loads in data that will be used to constrain the PDA. It’s supposed to be eventually more integrated with Pecan, which will know how to load all kinds of data from all kinds of sources. For now, it can do NEE from Ameriflux.</li>
<li><em>pda.define.llik.r</em> A simple helper function that defines likelihood functions for different datasets. Probably in the future this should be queried from the database or something. For now, it is extremely limited. The original test case of NEE assimilation uses a heteroskedastic Laplacian distribution.</li>
<li><em>pda.get.model.output.R</em> Another function that will eventually grow to handle many more cases, or perhaps be replaced by a better system altogether. For now though, it again just handles Ameriflux NEE.</li>
</ul>
</div>
<div id="get.da.data..r-plot.da.r" class="section level4">
<h4><span class="header-section-number">5.2.0.13</span> <strong>get.da.data.*.R, plot.da.R</strong></h4>
<p>Old codes written by Carl Davidson. Defunct now, but may contain good ideas so currently left in.</p>
</div>
<div id="sda" class="section level4">
<h4><span class="header-section-number">5.2.0.14</span> State data assimilation (SDA)</h4>
<p><code>sda.enkf.R</code> is housed within: <code>/pecan/modules/assim.sequential/R</code></p>
<p>The tree ring tutorial is housed within: <code>/pecan/documentation/tutorials/StateAssimilation</code></p>
<p>More descriptive SDA methods can be found at: <code>/pecan/book_source/adve_user_guide_web/SDA_Methods.Rmd</code></p>
</div>
<div id="sda.enkf.r-description" class="section level4">
<h4><span class="header-section-number">5.2.0.15</span> <strong>sda.enkf.R Description</strong></h4>
<p>This is the main ensemble Kalman filter and generalized filter code. Originally, this was just ensemble Kalman filter code. Mike Dietze and Ann Raiho added a generalized ensemble filter to avoid filter divergence. The output of this function will be all the of run outputs, a PDF of diagnostics, and an Rdata object that includes three lists:</p>
<ul>
<li>FORECAST will be the ensemble forecasts for each year</li>
<li>ANALYSIS will be the updated ensemble sample given the NPP observations</li>
<li>enkf.params contains the prior and posterior mean vector and covariance matrix for each time step.</li>
</ul>
</div>
<div id="sda.enkf.r-arguments" class="section level4">
<h4><span class="header-section-number">5.2.0.16</span> <strong>sda.enkf.R Arguments</strong></h4>
<ul>
<li><p>settings - (required) <a href="user-section.html#state-data-assimilation-tags-example">State Data Assimilation Tags Example</a> settings object</p></li>
<li><p>obs.mean - (required) a list of observation means named with dates in YYYY/MM/DD format</p></li>
<li><p>obs.cov - (required) a list of observation covariances names with dates in YYYY/MM/DD format</p></li>
<li><p>IC - (optional) initial condition matrix (dimensions: ensemble memeber # by state variables). Default is NULL.</p></li>
<li><p>Q - (optional) process covariance matrix (dimensions: state variable by state variables). Defualt is NULL.</p></li>
</ul>
</div>
<div id="state-data-assimilation-workflow" class="section level4">
<h4><span class="header-section-number">5.2.0.17</span> State Data Assimilation Workflow</h4>
<p>Before running sda.enkf, these tasks must be completed (in no particular order),</p>
<ul>
<li><p>Read in a <a href="user-section.html#state-data-assimilation-tags-example">State Data Assimilation Tags Example</a> settings file with tags listed below. i.e. read.settings(‘pecan.SDA.xml’)</p></li>
<li><p>Load data means (obs.mean) and covariances (obs.cov) as lists with PEcAn naming and unit conventions. Each observation must have a date in YYYY/MM/DD format (optional time) associated with it. If there are missing data, the date must still be represented in the list with an NA as the list object.</p></li>
<li><p>Create initial conditions matrix (IC) that is state variables columns by ensemble members rows in dimension. <a href="user-section.html#sample.ic.model.r">sample.IC.MODEL</a> can be used to create the IC matrix, but it is not required. This IC matrix is fed into write.configs for the initial model runs.</p></li>
</ul>
<p>The main parts of the SDA function are:</p>
<p>Setting up for initial runs:</p>
<ul>
<li><p>Set parameters</p></li>
<li><p>Load initial run inputs via <a href="user-section.html#split.inputs.model.r">split.inputs.MODEL</a></p></li>
<li><p>Open database connection</p></li>
<li><p>Get new workflow ids</p></li>
<li><p>Create ensemble ids</p></li>
</ul>
<p>Performing the initial set of runs</p>
<p>Set up for data assimilation</p>
<p>Loop over time</p>
<ul>
<li><p><a href="user-section.html#read.restart.model.r">read.restart.MODEL</a> - read model restart files corresponding to start.time and stop.time that you want to assimilate data into</p></li>
<li><p>Analysis - There are four choices based on if process variance is TRUE or FALSE and if there is data or not. <a href="user-section.html#analysis-options">See explaination below.</a></p></li>
<li><p><a href="user-section.html#write.restart.model.r">write.restart.MODEL</a> - This function has two jobs. First, to insert adjusted state back into model restart file. Second, to update start.time, stop.time, and job.sh.</p></li>
<li><p>run model</p></li>
</ul>
<p>Save outputs</p>
<p>Create diagnostics</p>
</div>
<div id="state-data-assimilation-tags-example" class="section level4">
<h4><span class="header-section-number">5.2.0.18</span> State Data Assimilation Tags Example</h4>
<pre><code>&lt;state.data.assimilation&gt;
  &lt;adjustment&gt;TRUE&lt;/adjustment&gt;
  &lt;process.variance&gt;FALSE&lt;/process.variance&gt;
  &lt;sample.parameters&gt;FALSE&lt;/sample.parameters&gt;
   &lt;state.variables&gt;
    &lt;variable&gt;
      &lt;variable.name&gt;AGB.pft&lt;/variable.name&gt;
      &lt;unit&gt;MgC/ha/yr&lt;/unit&gt;
      &lt;min_value&gt;0&lt;/min_value&gt;
      &lt;max_value&gt;100000000&lt;/max_value&gt;
    &lt;/variable&gt;
    &lt;variable&gt;
      &lt;variable.name&gt;TotSoilCarb&lt;/variable.name&gt;
      &lt;unit&gt;KgC/m^2&lt;/unit&gt;
      &lt;min_value&gt;0&lt;/min_value&gt;
      &lt;max_value&gt;100000000&lt;/max_value&gt;
    &lt;/variable&gt;
  &lt;/state.variables&gt;
  &lt;spin.up&gt;
    &lt;start.date&gt;1950/01/01&lt;/start.date&gt;
    &lt;end.date&gt;1960/12/31&lt;/end.date&gt;
  &lt;/spin.up&gt;
  &lt;forecast.time.step&gt;1&lt;/forecast.time.step&gt;
  &lt;start.date&gt;1961/01/01&lt;/start.date&gt;
  &lt;end.date&gt;2010/12/31&lt;/end.date&gt;
 &lt;/state.data.assimilation&gt;</code></pre>
</div>
<div id="state-data-assimilation-tags-descriptions" class="section level4">
<h4><span class="header-section-number">5.2.0.19</span> State Data Assimilation Tags Descriptions</h4>
<ul>
<li><strong>adjustment</strong> : [optional] TRUE/FLASE flag for if ensembles needs to be adjusted based on weights estimated given their likelihood during analysis step. The defualt is TRUE for this flag.</li>
<li><strong>process.variance</strong> : [optional] TRUE/FLASE flag for if process variance should be estimated (TRUE) or not (FALSE). If TRUE, a generalized ensemble filter will be used. If FALSE, an ensemble Kalman filter will be used. Default is FALSE. If you use the TRUE argument you can set three more optional tags to control the MCMCs built for the generalized esnsemble filter.</li>
<li><strong>nitrGEF</strong> : [optional] numeric defining the length of the MCMC chains.</li>
<li><strong>nthin</strong> : [optional] numeric defining thining length for the MCMC chains.</li>
<li><strong>nburnin</strong> : [optional] numeric defining the number of burnins during the MCMCs.</li>
<li><p><strong>censored.data</strong> : [optional] logical set TRUE for censored state variables.</p></li>
<li><strong>sample.parameters</strong> : [optional] TRUE/FLASE flag for if parameters should be sampled for each ensemble member or not. This allows for more spread in the initial conditions of the forecast.</li>
<li><strong>state.variable</strong> : [required] State variable that is to be assimilated (in PEcAn standard format).</li>
<li><strong>spin.up</strong> : [required] start.date and end.date for initial model runs.</li>
<li><strong><em>NOTE:</em></strong> start.date and end.date are distinct from values set in the run tag because initial runs can be done over a subset of the full run.</li>
<li><strong>forecast.time.step</strong> : [optional] In the future, this will be used to allow the forecast time step to vary from the data time step.</li>
<li><strong>start.date</strong> : [optional] start date of the state data assimilation (in YYYY/MM/DD format)</li>
<li><strong>end.date</strong> : [optional] end date of the state data assimilation (in YYYY/MM/DD format)</li>
<li><p><strong><em>NOTE:</em></strong> start.date and end.date are distinct from values set in the run tag because this analysis can be done over a subset of the run.</p></li>
</ul>
</div>
<div id="model-specific-functions-for-sda-workflow" class="section level4">
<h4><span class="header-section-number">5.2.0.20</span> Model Specific Functions for SDA Workflow</h4>
</div>
<div id="read.restart.model.r" class="section level4">
<h4><span class="header-section-number">5.2.0.21</span> read.restart.MODEL.R</h4>
<p>The purpose of read.restart is to read model restart files and return a matrix that is site rows by state variable columns. The state variables must be in PEcAn names and units. The arguments are:</p>
<ul>
<li><p>outdir - output directory</p></li>
<li><p>runid - ensemble member run ID</p></li>
<li><p>stop.time - used to determine which restart file to read (in POSIX format)</p></li>
<li><p>settings - <a href="user-section.html#state-data-assimilation-tags-example">pecan.SDA.xml</a> settings object</p></li>
<li><p>var.names - vector with state variable names with PEcAn standard naming. Example: c(‘AGB.pft’, ‘TotSoilCarb’)</p></li>
<li><p>params - parameters used by ensemble member (same format as write.configs)</p></li>
</ul>
</div>
<div id="write.restart.model.r" class="section level4">
<h4><span class="header-section-number">5.2.0.22</span> write.restart.MODEL.R</h4>
<p>This model specific function takes in new state and new parameter matrices from sda.enkf.R after the analysis step and translates new variables back to the model variables. Then, updates start.time, stop.time, and job.sh so that start.model.runs() does the correct runs with the new states. In write.restart.LINKAGES and write.restart.SIPNET, job.sh is updated by using write.configs.MODEL.</p>
<ul>
<li><p>outdir - output directory</p></li>
<li><p>runid - run ID for ensemble member</p></li>
<li><p>start.time - beginning of model run (in POSIX format)</p></li>
<li><p>stop.time - end of model run (in POSIX format)</p></li>
<li><p>settings - <a href="user-section.html#state-data-assimilation-tags-example">pecan.SDA.xml</a> settings object</p></li>
<li><p>new.state - matrix from analysis of updated state variables with PEcAn names (dimensions: site rows by state variables columns)</p></li>
<li><p>new.params - In the future, this will allow us to update parameters based on states (same format as write.configs)</p></li>
<li><p>inputs - model specific inputs from <a href="user-section.html#split.inputs.model.r">split.inputs.MODEL</a> used to run the model from start.time to stop.time</p></li>
<li><p>RENAME - [optional] Flag used in write.restart.LINKAGES.R for development.</p></li>
</ul>
</div>
<div id="split.inputs.model.r" class="section level4">
<h4><span class="header-section-number">5.2.0.23</span> split.inputs.MODEL.R</h4>
<p>This model specific function gives the correct met and/or other model inputs to settings<span class="math inline">\(run\)</span>inputs. This function returns settings<span class="math inline">\(run\)</span>inputs to an inputs argument in sda.enkf.R. But, the inputs will not need to change for all models and should return settings<span class="math inline">\(run\)</span>inputs unchanged if that is the case.</p>
<ul>
<li><p>settings - <a href="user-section.html#state-data-assimilation-tags-example">pecan.SDA.xml</a> settings object</p></li>
<li><p>start.time - start time for model run (in POSIX format)</p></li>
<li><p>stop.time - stop time for model run (in POSIX format)</p></li>
</ul>
</div>
<div id="sample.ic.model.r" class="section level4">
<h4><span class="header-section-number">5.2.0.24</span> sample.IC.MODEL.R</h4>
<p>This model specific function is optional. But, it can be used to create initial condition matrix (IC) with # state variables columns by # ensemble rows. This IC matrix is used for the initial runs in sda.enkf.R in the write.configs.MODEL function.</p>
<ul>
<li><p>ne - number of ensemble members</p></li>
<li><p>state - matrix of state variables to get initial conditions from</p></li>
<li><p>year - used to determine which year to sample initial conditions from</p></li>
</ul>
</div>
<div id="analysis-options" class="section level4">
<h4><span class="header-section-number">5.2.0.25</span> Analysis Options</h4>
<p>There are four options depending on whether process variance is TRUE/FALSE and whether or not there is data or not.</p>
<ul>
<li><p>If there is no data and process variance = FALSE, there is no analysis step.</p></li>
<li><p>If there is no data and process variance = TRUE, process variance is added to the forecast.</p></li>
<li><p>If there is data and process variance = TRUE, <a href="user-section.html#the-generalized-ensemble-filter">the generalized ensemble filter</a> is implemented with MCMC.</p></li>
<li><p>If there is data and process variance = FALSE, the Kalman filter is used and solved analytically.</p></li>
</ul>
</div>
<div id="the-generalized-ensemble-filter" class="section level4">
<h4><span class="header-section-number">5.2.0.26</span> The Generalized Ensemble Filter</h4>
<p>An ensemble filter is a sequential data assimilation algorithm with two procedures at every time step: a forecast followed by an analysis. The forecast ensembles arise from a model while the analysis makes an adjustment of the forecasts ensembles from the model towards the data. An ensemble Kalman filter is typically suggested for this type of analysis because of its computationally efficient analytical solution and its ability to update states based on an estimate of covariance structure. But, in some cases, the ensemble Kalman filter fails because of filter divergence. Filter divergence occurs when forecast variability is too small, which causes the analysis to favor the forecast and diverge from the data. Models often produce low forecast variability because there is little internal stochasticity. Our ensemble filter overcomes this problem in a Bayesian framework by including an estimation of model process variance. This methodology also maintains the benefits of the ensemble Kalman filter by updating the state vector based on the estimated covariance structure.</p>
<p>This process begins after the model is spun up to equilibrium.</p>
<p>The likelihood function uses the data vector <span class="math inline">\(\left(\boldsymbol{y_{t}}\right)\)</span> conditional on the estimated state vector <span class="math inline">\(\left(\boldsymbol{x_{t}}\right)\)</span> such that</p>
<p><span class="math inline">\(\boldsymbol{y}_{t}\sim\mathrm{multivariate\:normal}(\boldsymbol{x}_{t},\boldsymbol{R}_{t})\)</span></p>
<p>where <span class="math inline">\(\boldsymbol{R}_{t}=\boldsymbol{\sigma}_{t}^{2}\boldsymbol{I}\)</span> and <span class="math inline">\(\boldsymbol{\sigma}_{t}^{2}\)</span> is a vector of data variances. To obtain an estimate of the state vector <span class="math inline">\(\left(\boldsymbol{x}_{t}\right)\)</span>, we use a process model that incorporates a process covariance matrix <span class="math inline">\(\left(\boldsymbol{Q}_{t}\right)\)</span>. This process covariance matrix differentiates our methods from past ensemble filters. Our process model contains the following equations</p>
<p><span class="math inline">\(\boldsymbol{x}_{t} \sim \mathrm{multivariate\: normal}(\boldsymbol{x}_{model_{t}},\boldsymbol{Q}_{t})\)</span></p>
<p><span class="math inline">\(\boldsymbol{x}_{model_{t}} \sim \mathrm{multivariate\: normal}(\boldsymbol{\mu}_{forecast_{t}},\boldsymbol{P}_{forecast_{t}})\)</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}_{forecast_{t}}\)</span> is a vector of means from the ensemble forecasts and <span class="math inline">\(\boldsymbol{P}_{forecast_{t}}\)</span> is a covariance matrix calculated from the ensemble forecasts. The prior for our process covariance matrix is <span class="math inline">\(\boldsymbol{Q}_{t}\sim\mathrm{Wishart}(\boldsymbol{V}_{t},n_{t})\)</span> where <span class="math inline">\(\boldsymbol{V}_{t}\)</span> is a scale matrix and <span class="math inline">\(n_{t}\)</span> is the degrees of freedom. The prior shape parameters are updated at each time step through moment matching such that</p>
<p><span class="math inline">\(\boldsymbol{V}_{t+1} = n_{t}\bar{\boldsymbol{Q}}_{t}\)</span></p>
<p><span class="math inline">\(n_{t+1} = \frac{\sum_{i=1}^{I}\sum_{j=1}^{J}\frac{v_{ijt}^{2}+v_{iit}v_{jjt}}{Var(\boldsymbol{\bar{Q}}_{t})}}{I\times J}\)</span></p>
<p>where we calculate the mean of the process covariance matrix <span class="math inline">\(\left(\bar{\boldsymbol{Q}_{t}}\right)\)</span> from the posterior samples at time t. Degrees of freedom for the Wishart are typically calculated element by element where <span class="math inline">\(v_{ij}\)</span> are the elements of <span class="math inline">\(\boldsymbol{V}_{t}\)</span>. <span class="math inline">\(I\)</span> and <span class="math inline">\(J\)</span> index rows and columns of <span class="math inline">\(\boldsymbol{V}\)</span>. Here, we calculate a mean number of degrees of freedom for <span class="math inline">\(t+1\)</span> by summing over all the elements of the scale matrix <span class="math inline">\(\left(\boldsymbol{V}\right)\)</span> and dividing by the count of those elements <span class="math inline">\(\left(I\times J\right)\)</span>. We fit this model sequentially through time in the R computing environment using R package ‘rjags.’</p>
</div>
<div id="multi-site-state-data-assimilation." class="section level4">
<h4><span class="header-section-number">5.2.0.27</span> Multi-site State data assimilation.</h4>
<p><code>sda.enkf.multisite</code> function allows for assimilation of observed data at multiple sites at the same time. In order to run a multi-site SDA, one needs to send a multisettings pecan xml file to this function. This multisettings xml file needs to contain information required for running at least two sites under <code>run</code> tag. The code will automatically run the ensembles for all the sites and reformats the outputs matching the required formats for analysis step.</p>
<p>The observed mean and cov needs to be formatted as list of different dates with observations. For each element of this list also there needs to be a list with mean and cov matrices of different sites named by their siteid. This would look like something like this:</p>
<pre><code>&gt; obs.mean

$`2010/12/31`
$`2010/12/31`$`1000000650`
   AbvGrndWood     GWBI
    111.502    1.0746

$`2010/12/31`$`1000000651`
   AbvGrndWood     GWBI
    114.302    1.574695</code></pre>
<pre><code>&gt; obs.cov

$`2010/12/31`
$`2010/12/31`$`1000000650`
           [,1]        [,2]
[1,] 19.7821691 0.213584319
[2,]  0.5135843 0.005162113

$`2010/12/31`$`1000000651`
           [,1]        [,2]
[1,] 15.2821691 0.513584319
[2,]  0.1213583 0.001162113</code></pre>
<p>An example of multi-settings pecan xml file also may look like below:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;pecan.multi&gt;
 &lt;state.data.assimilation&gt;
   &lt;process.variance&gt;FALSE&lt;/process.variance&gt;
   &lt;adjustment&gt;TRUE&lt;/adjustment&gt;
   &lt;data&gt;
    &lt;format_id&gt;1000000040&lt;/format_id&gt;
    &lt;input.id&gt;1000013298&lt;/input.id&gt;
  &lt;/data&gt;
   &lt;state.variables&gt;
   &lt;variable&gt;
      &lt;variable.name&gt;GWBI&lt;/variable.name&gt;
   &lt;unit&gt;KgC/m^2&lt;/unit&gt;
       &lt;min_value&gt;0&lt;/min_value&gt;
       &lt;max_value&gt;9999&lt;/max_value&gt;
   &lt;/variable&gt;
   &lt;variable&gt;
   &lt;variable.name&gt;AbvGrndWood&lt;/variable.name&gt;
   &lt;unit&gt;KgC/m^2&lt;/unit&gt;
   &lt;min_value&gt;0&lt;/min_value&gt;
   &lt;max_value&gt;9999&lt;/max_value&gt;
   &lt;/variable&gt;
   &lt;/state.variables&gt;
  &lt;start.date&gt;1960/01/01&lt;/start.date&gt;
  &lt;end.date&gt;2000/12/31&lt;/end.date&gt;
  &lt;/state.data.assimilation&gt;
 &lt;info&gt;
    &lt;notes&gt;&lt;/notes&gt;
    &lt;userid&gt;-1&lt;/userid&gt;
    &lt;username&gt;&lt;/username&gt;
    &lt;date&gt;2017/12/06 21:19:33 +0000&lt;/date&gt;
  &lt;/info&gt;
 &lt;outdir&gt;/fs/data3/hamzed/output/MultiSite_Sandbox/paleon_sda_SIPNET-8768&lt;/outdir&gt;
 &lt;database&gt;
    &lt;bety&gt;
      &lt;user&gt;bety&lt;/user&gt;
      &lt;password&gt;bety&lt;/password&gt;
      &lt;host&gt;128.197.168.114&lt;/host&gt;
      &lt;dbname&gt;bety&lt;/dbname&gt;
      &lt;driver&gt;PostgreSQL&lt;/driver&gt;
      &lt;write&gt;false&lt;/write&gt;
    &lt;/bety&gt;
    &lt;dbfiles&gt;/fs/data1/pecan.data/dbfiles/&lt;/dbfiles&gt;
  &lt;/database&gt;
 &lt;pfts&gt;
  &lt;pft&gt;
   &lt;name&gt;temperate.deciduous_SDA&lt;/name&gt;
   &lt;constants&gt;
    &lt;num&gt;2&lt;/num&gt;
   &lt;/constants&gt;
   &lt;outdir&gt;/fs/data2/output//PEcAn_1000008768/pft/temperate.deciduous_SDA&lt;/outdir&gt;
   &lt;posteriorid&gt;1000008552&lt;/posteriorid&gt;
  &lt;/pft&gt;
 &lt;/pfts&gt;
 &lt;meta.analysis&gt;
    &lt;iter&gt;3000&lt;/iter&gt;
    &lt;random.effects&gt;FALSE&lt;/random.effects&gt;
  &lt;/meta.analysis&gt;
 &lt;ensemble&gt;
  &lt;size&gt;20&lt;/size&gt;
  &lt;ensemble.id&gt;1000016146&lt;/ensemble.id&gt;
  &lt;start.year&gt;1995&lt;/start.year&gt;
  &lt;end.year&gt;1999&lt;/end.year&gt;
  &lt;samplingspace&gt;
  &lt;parameters&gt;
    &lt;method&gt;uniform&lt;/method&gt;
  &lt;/parameters&gt;
  &lt;met&gt;
    &lt;method&gt;sampling&lt;/method&gt;
  &lt;/met&gt;
  &lt;soil&gt;    
  &lt;parent&gt;parameters&lt;/parent&gt;
  &lt;/soil&gt;
  &lt;vegetation&gt;
  &lt;parent&gt;soil&lt;/parent&gt;
  &lt;/vegetation&gt;
  &lt;/samplingspace&gt;
 &lt;/ensemble&gt;
 &lt;model&gt;
  &lt;id&gt;1000000022&lt;/id&gt;
  &lt;default.param&gt;/fs/data3/hamzed/output/paleon_sda_SIPNET-8768/Bartlett.param&lt;/default.param&gt;
  &lt;type&gt;SIPNET&lt;/type&gt;
  &lt;revision&gt;r136&lt;/revision&gt;
  &lt;delete.raw&gt;FALSE&lt;/delete.raw&gt;
  &lt;binary&gt;/fs/data5/pecan.models/SIPNET/trunk/sipnet_ssr&lt;/binary&gt;
 &lt;/model&gt;
 &lt;workflow&gt;
    &lt;id&gt;1000008768&lt;/id&gt;
  &lt;/workflow&gt;
 &lt;run&gt;
  &lt;settings.1000000650&gt;
  &lt;site&gt;
   &lt;id&gt;1000000650&lt;/id&gt;
   &lt;met.start&gt;1960/01/01&lt;/met.start&gt;
   &lt;met.end&gt;1965/12/31&lt;/met.end&gt;
   &lt;name&gt;Harvard Forest - Lyford Plots (PalEON PHA)&lt;/name&gt;
   &lt;lat&gt;42.53&lt;/lat&gt;
   &lt;lon&gt;-72.18&lt;/lon&gt;
  &lt;/site&gt;
  &lt;inputs&gt;
   &lt;met&gt;
    &lt;source&gt;CRUNCEP&lt;/source&gt;
    &lt;output&gt;SIPNET&lt;/output&gt;
    &lt;path&gt;
    &lt;path1&gt;/fs/data1/pecan.data/dbfiles/CRUNCEP_SIPNET_site_0-758/CRUNCEP.1960-01-01.2010-12-31.clim&lt;/path1&gt;
    &lt;/path&gt;
   &lt;/met&gt;
  &lt;/inputs&gt;
  &lt;start.date&gt;1960/01/01&lt;/start.date&gt;
  &lt;end.date&gt;1980/12/31&lt;/end.date&gt;
  &lt;/settings.1000000650&gt;
  &lt;settings.1000000651&gt;
  &lt;site&gt;
   &lt;id&gt;1000000651&lt;/id&gt;
   &lt;met.start&gt;1960/01/01&lt;/met.start&gt;
   &lt;met.end&gt;1965/12/31&lt;/met.end&gt;
   &lt;name&gt;Harvard Forest - Lyford Plots (PalEON PHA)&lt;/name&gt;
   &lt;lat&gt;42.53&lt;/lat&gt;
   &lt;lon&gt;-72.18&lt;/lon&gt;
  &lt;/site&gt;
  &lt;inputs&gt;
   &lt;met&gt;
    &lt;source&gt;CRUNCEP&lt;/source&gt;
    &lt;output&gt;SIPNET&lt;/output&gt;
    &lt;path&gt;
    &lt;path1&gt;/fs/data1/pecan.data/dbfiles/CRUNCEP_SIPNET_site_0-758/CRUNCEP.1960-01-01.2010-12-31.clim&lt;/path1&gt;
    &lt;/path&gt;
   &lt;/met&gt;
  &lt;/inputs&gt;
  &lt;start.date&gt;1960/01/01&lt;/start.date&gt;
  &lt;end.date&gt;1980/12/31&lt;/end.date&gt;
  &lt;/settings.1000000651&gt;
 &lt;/run&gt;
 &lt;host&gt;
  &lt;name&gt;localhost&lt;/name&gt;
  &lt;rundir&gt;/fs/data3/hamzed/output/MultiSite_Sandbox/paleon_sda_SIPNET-8768/run&lt;/rundir&gt;
  &lt;outdir&gt;/fs/data3/hamzed/output/MultiSite_Sandbox/paleon_sda_SIPNET-8768/out&lt;/outdir&gt;
 &lt;/host&gt;
 &lt;settings.info&gt;
  &lt;deprecated.settings.fixed&gt;TRUE&lt;/deprecated.settings.fixed&gt;
  &lt;settings.updated&gt;TRUE&lt;/settings.updated&gt;
  &lt;checked&gt;TRUE&lt;/checked&gt;
 &lt;/settings.info&gt;
 &lt;rundir&gt;/fs/data3/hamzed/output/MultiSite_Sandbox/paleon_sda_SIPNET-8768/run&lt;/rundir&gt;
 &lt;modeloutdir&gt;/fs/data3/hamzed/output/MultiSite_Sandbox/paleon_sda_SIPNET-8768/out&lt;/modeloutdir&gt;
 &lt;multisettings&gt;run&lt;/multisettings&gt;
&lt;/pecan.multi&gt;</code></pre>
<p>==&gt; 03_hidden_analyses/03_sda_methods.Rmd &lt;==
#### State Data Assimilation Methods</p>
<p><em>By Ann Raiho</em></p>
<p>Our goal is build a fully generalizable state data assimilation (SDA) workflow that will assimilate multiple types of data and data products into ecosystem models within PEcAn temporally and spatially. But, during development, specifically with PalEON goals in mind, we have been focusing on assimilating tree ring estimated NPP and AGB and pollen derived fractional composition into two ecosystem models, SIPNET and LINKAGES, at Harvard Forest. This methodology will soon be expanded to include the PalEON sites listed on the <a href="https://paleon.geography.wisc.edu/doku.php/working_groups;state_data_assimilation">state data assimilation wiki page</a>.</p>
</div>
<div id="data-products" class="section level4">
<h4><span class="header-section-number">5.2.0.28</span> Data Products</h4>
<p>During workflow development, we have been working with tree ring estimated NPP and AGB and pollen derived fractional composition data products. Both of these data products have been estimated with a full accounting of uncertainty, which provides us with state variable observation mean vector and covariance matrix at each time step. These data products are discussed in more detail below. Even though we have been working with specific data products during development, our workflow is generalizable to alternative data products as long as we can calculate a state variable observation mean vector and covariance for a time point.</p>
</div>
<div id="tree-rings" class="section level4">
<h4><span class="header-section-number">5.2.0.29</span> Tree Rings</h4>
<p>We have been primarily been working with the tree ring data product created by Andria Dawson and Chris Paciorek and the PEcAn tree ring allometry module. They have developed a Bayesian model that estimates annual aboveground biomass increment (Mg/ha/yr) and aboveground biomass (Mg/ha) for each tree in a dataset. We obtain this data and aggregate to the level appropriate for the ecosystem model. In SIPNET, we are assimilating annual gross woody increment (Mg/ha/yr) and above ground woody biomass (Mg/ha). In LINKAGES, we are assimilating annual species biomass. More information on deriving these tree ring data products can be found in Dawson et al 201?.</p>
<p>We have been working mostly with tree data collected at Harvard Forest. Tree rings and census data were collected at Lyford Plot between 1960 and 2010 in three separate plots. Other tree ring data will be added to this analysis in the future from past PEON courses (UNDERC), Kelly Heilman (Billy’s Lake and Bigwoods), and Alex Dye (Huron Mt. Club).</p>
</div>
<div id="pollen" class="section level4">
<h4><span class="header-section-number">5.2.0.30</span> Pollen</h4>
<p>STEPPS is a Bayesian model developed by Paciorek and McLachlan 2009 and Dawson et al 2016 to estimate spatially gridded fractional composition from fossil pollen. We have been working with STEPPS1 output, specifically with the grid cell that contains Harvard Forest. The temporal resolution of this data product is centennial. Our workflow currently operates at annual time steps, but does not require data at every time step. So, it is possible to assimilate fractional composition every one hundred years or to assimilate fractional composition data every year by accounting for variance inflation.</p>
<p>In the future, pollen derived biomass (ReFAB) will also be available for data assimilation. Although, we have not discussed how STEPPS and ReFAB data assimilation will work.</p>
</div>
<div id="variance-inflation" class="section level4">
<h4><span class="header-section-number">5.2.0.31</span> Variance Inflation</h4>
<p>*Side Note: Probably want to call this something else now.</p>
<p>Since the fractional composition data product has a centennial resolution, in order to use fractional composition information every year we need to change the weight the data has on the analysis. The basic idea is to downweight the likelihood relative to the prior to account for (a) the fact that we assimilate an observation multiple times and (b) the fact that the number of STEPPS observations is ‘inflated’ because of the autocorrelation. To do this, we take the likelihood and raise it to the power of (1/w) where ‘w’ is an inflation factor.</p>
<p>w = D * (N / ESS)</p>
<p>where D is the length of the time step. In our case D = 100. N is the number of time steps. In our case N = 11. and ESS is the effective sample size. The ESS is calculated with the following function where ntimes is the same as N above and sims is a matrix with the dimensions number of MCMC samples by number of state variables.</p>
<pre><code>ESS_calc &lt;- function(ntimes, sims){
        # center based on mean at each time to remove baseline temporal correlation 
        # (we want to estimate effective sample size effect from correlation of the errors)
        row.means.sims &lt;- sims - rowMeans(sims)  
        
        # compute all pairwise covariances at different times
        covars &lt;- NULL
        for(lag in 1:(ntimes-1)){
          covars &lt;- c(covars, rowMeans(row.means.sims[(lag+1):ntimes, , drop = FALSE] * row.means.sims[1:(ntimes-lag), , drop = FALSE])) 
        }
        vars &lt;- apply(row.means.sims, 1, var) # pointwise post variances at each time, might not be homoscedastic
        
        # nominal sample size scaled by ratio of variance of an average
        # under independence to variance of average of correlated values
        neff &lt;- ntimes * sum(vars) / (sum(vars) + 2 * sum(covars))
        return(neff)
      }</code></pre>
<p>The ESS for the STEPPS1 data product is 3.6, so w in our assimilation of fractional composition at Harvard Forest will be w = 305.6.</p>
</div>
<div id="current-models" class="section level4">
<h4><span class="header-section-number">5.2.0.32</span> Current Models</h4>
<p>SIPNET and LINKAGES are the two ecosystem models that have been used during state data assimilation development within PEcAn. SIPNET is a simple ecosystem model that was built for… LINKAGES is a forest gap model created to simulate the process of succession that occurs when a gap is opened in the forest canopy. LINKAGES has 72 species level plant functional types and the ability to simulate some below ground processes (C and N cycles).</p>
</div>
<div id="model-calibration" class="section level4">
<h4><span class="header-section-number">5.2.0.33</span> Model Calibration</h4>
<p>Without model calibration both SIPNET and LINKAGES make incorrect predictions about Harvard Forest. To confront this problem, SIPNET and LINKAGES will both be calibrated using data collected at the Harvard Forest flux tower. Istem has completed calibration for SIPNET using a <a href="https://github.com/PecanProject/pecan/blob/develop/modules/assim.batch/R/pda.emulator.R">parameter data assimilation emulator</a> contained within the PEcAn workflow. LINKAGES will also be calibrated using this method. This method is also generalizable to other sites assuming there is data independent of data assimilation data available to calibrate against.</p>
</div>
<div id="initial-conditions" class="section level4">
<h4><span class="header-section-number">5.2.0.34</span> Initial Conditions</h4>
<p>The initial conditions for SIPNET are sampled across state space based on data distributions at the time when the data assimilation will begin. We do not sample LINAKGES for initial conditions and instead perform model spin up for 100 years prior to beginning data assimilation. In the future, we would like to estimate initial conditions based on data. We achieve adequate spread in the initial conditions by allowing the parameters to vary across ensemble members.</p>
</div>
<div id="drivers" class="section level4">
<h4><span class="header-section-number">5.2.0.35</span> Drivers</h4>
<p>We are currently using Global Climate Model (GCM) drivers from the PaLEON model intercomparison. Christy Rollinson and John Tipton are creating MET downscaled GCM drivers for the Paleon data assimilation sites. We will use these drivers when they are available because they are a closer representation of reality.</p>
</div>
<div id="sequential-state-data-assimilation" class="section level4">
<h4><span class="header-section-number">5.2.0.36</span> Sequential State Data Assimilation</h4>
<p>We are using sequential state data assimilation methods to assimilate paleon data products into ecosystem models because less computation power is required for sequential state data assimilation than for particle filter methods.</p>
</div>
<div id="general-description" class="section level4">
<h4><span class="header-section-number">5.2.0.37</span> General Description</h4>
<p>The general sequential data assimilation framework consists of three steps at each time step:
1. Read the state variable output for time t from the model forecast ensembles and save the forecast mean (muf) and covariance (Pf).
2. If there are data mean (y) and covariance (R) at this time step, perform data assimilation analysis (either EnKF or generalized ensemble filter) to calculate the new mean (mua) and covariance (Pa) of the state variables.
3. Use mua and Pa to restart and run the ecosystem model ensembles with new state variables for time t+1.</p>
</div>
<div id="enkf" class="section level4">
<h4><span class="header-section-number">5.2.0.38</span> EnKF</h4>
<p>There are two ways to implement sequential state data assimilation at this time. The first is the Ensemble Kalman Filter (EnKF). EnKF has an analytical solution, so the kalman gain, analysis mean vector, and analysis covariance matrix can be calculated directly:</p>
<pre><code>       
        K &lt;- Pf %*% t(H) %*% solve((R + H %*% Pf %*% t(H))) ## Kalman Gain
        
        mu.a &lt;- mu.f + K %*% (Y - H %*% mu.f) # Analysis mean vector
        
        Pa   &lt;- (diag(ncol(X)) - K %*% H) %*% Pf # Analysis covariance matrix
        </code></pre>
<p>The EnKF is typically used for sequential state data assimilation, but we found that EnKF lead to filter divergence when combined with our uncertain data products. Filter divergence led us to create a generalized ensemble filter that estimates process variance.</p>
</div>
<div id="generalized-ensemble-filter" class="section level4">
<h4><span class="header-section-number">5.2.0.39</span> Generalized Ensemble Filter</h4>
<p>The generalized ensemble filter follows generally the three steps of sequential state data assimilation. But, in the generalized ensemble filter we add a latent state vector that accounts for added process variance. Furthermore, instead of solving the analysis analytically like the EnKF, we have to estimate the mean analysis vector and covariance matrix with MCMC.</p>
</div>
<div id="mapping-ensemble-output-to-tobit-space" class="section level4">
<h4><span class="header-section-number">5.2.0.40</span> Mapping Ensemble Output to Tobit Space</h4>
<p>There are some instances when we have right or left censored variables from the model forecast. For example, a model estimating species level biomass may have several ensemble members that produce zero biomass for a given species. We are considering this case a left censored state variable that needs to be mapped to normal space using a tobit model. We do this by creating two matrices with dimensions number of ensembles by state variable. The first matrix is a matrix of indicator variables (y.ind), and the second is a matrix of censored variables (y.censored). When the indicator variable is 0 the state variable (j) for ensemble member (i) is sampled. This allows us to impute a normal distribution for each state variable that contains ‘missing’ forecasts or forecasts of zero.</p>
<pre><code>tobit2space.model &lt;- nimbleCode({
    for(i in 1:N){
      y.censored[i,1:J] ~ dmnorm(muf[1:J], cov = pf[1:J,1:J])
      for(j in 1:J){
        y.ind[i,j] ~ dconstraint(y.censored[i,j] &gt; 0)
      }
    }
    
    muf[1:J] ~ dmnorm(mean = mu_0[1:J], cov = pf[1:J,1:J])
    
    Sigma[1:J,1:J] &lt;- lambda_0[1:J,1:J]/nu_0
    pf[1:J,1:J] ~ dinvwish(S = Sigma[1:J,1:J], df = J)
    
  })</code></pre>
</div>
<div id="generalized-ensemble-filter-model-description" class="section level4">
<h4><span class="header-section-number">5.2.0.41</span> Generalized Ensemble Filter Model Description</h4>
<p>Below is the BUGS code for the full analysis model. The forecast mean an covariance are calculated from the tobit2space model above. We use a tobit likelihood in this model because there are instances when the data may be left or right censored. Process variance is included by adding a latent model state (X) with a process precision matrix (q). We update our prior on q at each time step using our estimate of q from the previous time step.</p>
<pre><code>  tobit.model &lt;- nimbleCode({ 
    
    q[1:N,1:N]  ~ dwish(R = aq[1:N,1:N], df = bq) ## aq and bq are estimated over time
    Q[1:N,1:N] &lt;- inverse(q[1:N,1:N])
    X.mod[1:N] ~ dmnorm(muf[1:N], prec = pf[1:N,1:N]) ## Model Forecast ##muf and pf are assigned from ensembles
    
    ## add process error
    X[1:N]  ~ dmnorm(X.mod[1:N], prec = q[1:N,1:N])
    
    #agb linear
    #y_star[1:YN,1:YN] &lt;- X[1:YN,1:YN] #[choose]
    
    #f.comp non linear
    #y_star[1:YN] &lt;- X[1:YN] / sum(X[1:YN])
    
    ## Analysis
    y.censored[1:YN] ~ dmnorm(X[1:YN], prec = r[1:YN,1:YN]) #is it an okay assumpution to just have X and Y in the same order?
    
    #don&#39;t flag y.censored as data, y.censored in inits
    #remove y.censored samplers and only assign univariate samplers on NAs
    
    for(i in 1:YN){
      y.ind[i] ~ dconstraint(y.censored[i] &gt; 0)
    }
    
  })</code></pre>
</div>
<div id="ensemble-adjustment" class="section level4">
<h4><span class="header-section-number">5.2.0.42</span> Ensemble Adjustment</h4>
<p>Each ensemble member has a different set of species parameters. We adjust the updated state variables by using an ensemble adjustment. The ensemble adjustment weights the ensemble members based on their likelihood during the analysis step.</p>
<pre><code>      S_f  &lt;- svd(Pf)
      L_f  &lt;- S_f$d
      V_f  &lt;- S_f$v
      
      ## normalize
      Z &lt;- X*0
      for(i in seq_len(nrow(X))){
          Z[i,] &lt;- 1/sqrt(L_f) * t(V_f)%*%(X[i,]-mu.f)
      }
      Z[is.na(Z)]&lt;-0
      
      ## analysis
      S_a  &lt;- svd(Pa)
      L_a  &lt;- S_a$d
      V_a  &lt;- S_a$v
      
      ## analysis ensemble
      X_a &lt;- X*0
      for(i in seq_len(nrow(X))){
        X_a[i,] &lt;- V_a %*%diag(sqrt(L_a))%*%Z[i,] + mu.a
      }</code></pre>
</div>
<div id="diagnostics" class="section level4">
<h4><span class="header-section-number">5.2.0.43</span> Diagnostics</h4>
<p>There are three diagnostics we have currently implemented: time series, bias time series, and process variance. The time series diagnostics show the data, forecast, and analysis time series for each state variable. These are useful for visually assessing variance and magnitude of change of state variables through time. These time series are also updated throughout the analysis and are also created as a pdf at the end of the SDA workflow. There are two types of bias time series the first assess the bias in the update (the forecast minus the analysis) and the second assess the bias in the error (the forecast minus the data). These bias time series are useful for identifying which state variables have intrinsic bias within the model. For example, if red oak biomass in LINKAGES increases at every time step (the update and the error are always positive), this would suggest that LINKAGES has a positive growth or recruitment bias for red oak. Finally, when using the generalized ensemble filter to estimate process variance, there are two additional plots to assess estimation of process variance. The first is a correlation plot of the process covariance matrix. This tells us what correlations are incorrectly represented by the model. For example, if red oak biomass and white pine biomass are highly negatively correlated in the process covariance matrix, this means that the model either 1) has no relationship between red oak and white pine and they should affect each other negatively or 2) there is a positive relationship between red oak and white pine and there shouldn’t be any relationship. We can determine which of these is true by comparing the process covariance matrix to the model covariance matrix. The second process variance diagnostic plot shows how the degrees of freedom associated with estimating the process covariance matrix have changed through time. This plot should show increasing degrees of freedom through time.</p>
</div>
<div id="multisettings" class="section level4">
<h4><span class="header-section-number">5.2.0.44</span> MultiSettings</h4>
<p>(TODO: Under construction…)</p>
</div>
<div id="benchmarking" class="section level4">
<h4><span class="header-section-number">5.2.0.45</span> Benchmarking</h4>
<p>Benchmarking is the process of comparing model outputs against either experimental data or against other model outputs as a way to validate model performance.
We have a suit of statistical comparisons that provide benchmarking scores as well as visual comparisons that help in diagnosing data-model and/or model-model differences.</p>
</div>
<div id="data-preparation" class="section level4">
<h4><span class="header-section-number">5.2.0.46</span> Data Preparation</h4>
<p>All data that you want to compare with model runs must be registered in the database.
This is currently a step that must be done by hand either from the command line or through the online BETY interface.
The data must have three records:</p>
<ol style="list-style-type: decimal">
<li><p>An input record (Instructions <a href="user-section.html#NewInput">here</a>)</p></li>
<li><p>A database file record (Instructions <a href="user-section.html#NewInput">here</a>)</p></li>
<li><p>A format record (Instructions <a href="user-section.html#NewFormat">here</a>)</p></li>
</ol>
</div>
<div id="model-runs" class="section level4">
<h4><span class="header-section-number">5.2.0.47</span> Model Runs</h4>
<p>Model runs can be setup and executed
- Using the PEcAn web interface online or with a VM (<a href="#GettingStarted">see setup</a>)
- By hand using the <a href="pecanXML.html#pecanXML">pecan.xml</a></p>
</div>
<div id="the-benchmarking-shiny-app" class="section level4">
<h4><span class="header-section-number">5.2.0.48</span> The Benchmarking Shiny App</h4>
<p>The entire benchmarking process can be done through the Benchmarking R Shiny app.</p>
<p>When the model run has completed, navigate to the workflow visualization Shiny app.</p>
<ul>
<li>Load model data
<ul>
<li>Select the workflow and run id</li>
<li>Make sure that your model output is loading properly (i.e. you can see plots of your data)</li>
</ul></li>
<li>Load benchmarking data
<ul>
<li>Again make sure that you can see the uploaded data plotted alongside the model output. In the future there will be more tools for double checking that your uploaded data is appropriate for benchmarking, but for now you may need to do the sanity checks by hand.</li>
</ul></li>
</ul>
</div>
<div id="create-a-reference-run-record" class="section level4">
<h4><span class="header-section-number">5.2.0.49</span> Create a reference run record</h4>
<ul>
<li>Navigate to the Benchmarking tab
<ul>
<li>The first step is to register the new model run as a reference run in the database. Benchmarking cannot be done before this step is completed. When the reference run record has been created, additional menus for benchmarking will appear.</li>
</ul></li>
</ul>
</div>
<div id="setup-benchmarks-and-metrics" class="section level4">
<h4><span class="header-section-number">5.2.0.50</span> Setup Benchmarks and metrics</h4>
<ul>
<li>From the menus select
<ul>
<li>The variables in the uploaded data that you wish to compare with model output.</li>
<li>The numerical metrics you would like to use in your comparison.</li>
<li>Additional comparison plots that you would like to see.</li>
</ul></li>
<li>Note: All these selections populate the benchmarking section of the <code>pecan.BENCH.xml</code> which is then saved in the same location as the original run output. This xml is purely for reference.</li>
</ul>
<div id="benchmarking-output" class="section level5">
<h5><span class="header-section-number">5.2.0.50.1</span> Benchmarking Output</h5>
<ul>
<li>All benchmarking results are stored in the benchmarking directory which is created in the same folder as the original model run.</li>
<li>The benchmaking directory contains subdirectories for each of the datasets compared with the model output. The names of these directories are the same as the corresponding data set’s input id in BETY.</li>
<li>Each input directory contains <code>benchmarking.output.Rdata</code>, an Rdata file contianing all the results of the benchmarking workflow. <code>load(benchmarking.output.Rdata)</code> loads a list called <code>result.out</code> which contains the following:
<ul>
<li><code>bench.results</code>: a data frame of all numeric benchmarking scores</li>
<li><code>format</code>: a data frame that can be used to see how the input data was transformed to make it comparable to the model output. This involves converting from the original variable names and units to the internal pecan standard.</li>
<li><code>aligned.dat</code>: a data frame of the final aligned model and input values.</li>
</ul></li>
<li><p>All plots are saved as pdf files with names with “benchmark_plot-type_variable_input-id.pdf”</p></li>
<li><p>To view interactive results, naviage to the Benchmarking Plots tab in the shiny app.</p></li>
</ul>
</div>
</div>
<div id="benchmarking-in-pecan.xml" class="section level4">
<h4><span class="header-section-number">5.2.0.51</span> Benchmarking in pecan.xml</h4>
<p>Before reading this section, it is recommended that you <a href="pecanXML.html#pecanXML">familiarize yourself with basics of the pecan.xml file.</a></p>
<p>The <code>pecan.xml</code> has an <em>optional</em> benchmarking section. Below are all the tags in the benchmarking section explained. Many of these field are filled in automatically during the benchmarking process when using the benchmarking shiny app.</p>
<p>The only time one should edit the benchmarking section by hand is for performing clone runs. See <a href="#CloneRun">clone run documentation.</a></p>
<p><code>&lt;benchmarking&gt;</code> settings:</p>
<ul>
<li><code>ensemble_id</code>: the id of the ensemble that you will be using - the settings from this ensemble will be saved in a reference run record and then <code>ensemble_id</code> will be replaced with <code>reference_run_id</code></li>
<li><code>new_run</code>: TRUE = create new run, FALSE = use existing run (required, default FALSE)</li>
</ul>
<p>It is possible to look at more than one benchmark with a particular run.
The specific settings related to each benchmark are in a sub section called <code>benchmark</code></p>
<ul>
<li><code>input_id</code>: the id of the benchmarking data (required)</li>
<li><code>variable_id</code>: the id of the variable of interest within the data. If you leave this blank, all variables that are shared between the input and model output will be used.</li>
<li><code>metric_id</code>: the id(s) of the metric(s) to be calculated. If you leave this blank, all metrics will be used.</li>
</ul>
<p>Example:
In this example,
- we are using a pre-existing run from <code>ensemble_id = 1000010983</code> (<code>new_run = FALSE</code>)
- the output will be compared to data from <code>input_id = 1000013743</code>, specifically two variables of interest: <code>variable_id = 411, variable_id = 18</code>
- for <code>variable_id = 411</code> we will perform only one metric of comparison <code>metric_id = 1000000001</code>
- for for <code>variable_id = 18</code> we will perform two metrics of comparison <code>metric_id = 1000000001, metric_id = 1000000002</code></p>
<pre class="sourceCode xml"><code class="sourceCode xml"><span class="kw">&lt;benchmarking&gt;</span>
  <span class="kw">&lt;ensemble_id&gt;</span>1000010983<span class="kw">&lt;/ensemble_id&gt;</span>
  <span class="kw">&lt;new_run&gt;</span>FALSE<span class="kw">&lt;/new_run&gt;</span>
  <span class="kw">&lt;benchmark&gt;</span>
   <span class="kw">&lt;input_id&gt;</span>1000013743<span class="kw">&lt;/input_id&gt;</span>
   <span class="kw">&lt;variable_id&gt;</span>411<span class="kw">&lt;/variable_id&gt;</span>
   <span class="kw">&lt;site_id&gt;</span>853<span class="kw">&lt;/site_id&gt;</span>
   <span class="kw">&lt;metrics&gt;</span>
    <span class="kw">&lt;metric_id&gt;</span>1000000001<span class="kw">&lt;/metric_id&gt;</span>
   <span class="kw">&lt;/metrics&gt;</span>
  <span class="kw">&lt;/benchmark&gt;</span>
  <span class="kw">&lt;benchmark&gt;</span>
   <span class="kw">&lt;input_id&gt;</span>1000013743<span class="kw">&lt;/input_id&gt;</span>
   <span class="kw">&lt;variable_id&gt;</span>18<span class="kw">&lt;/variable_id&gt;</span>
   <span class="kw">&lt;site_id&gt;</span>853<span class="kw">&lt;/site_id&gt;</span>
   <span class="kw">&lt;metrics&gt;</span>
    <span class="kw">&lt;metric_id&gt;</span>1000000001<span class="kw">&lt;/metric_id&gt;</span>
    <span class="kw">&lt;metric_id&gt;</span>1000000002<span class="kw">&lt;/metric_id&gt;</span>
   <span class="kw">&lt;/metrics&gt;</span>
  <span class="kw">&lt;/benchmark&gt;</span>
<span class="kw">&lt;/benchmarking&gt;</span></code></pre>

</div>
<div id="pecan-remote" class="section level4">
<h4><span class="header-section-number">5.2.0.52</span> Remote execution with PEcAn</h4>
<p>Remote execution allows the user to leverage the power and storage of high performance computing clusters, AWS instances, or specially configured virtual machines, but without leaving their local working environment.
PEcAn uses remote execution primarily to run ecosystem models.</p>
<p>The infrastructure for remote execution lives in the <code>PEcAn.remote</code> package (<code>base/remote</code> in the PEcAn repository).</p>
<p>This section describes the following:</p>
<ol style="list-style-type: decimal">
<li>Checking capabilities to connect to the remote machine correctly:</li>
</ol>
<ul>
<li>Basics of command line SSH</li>
<li>SSH authentication with keys and passwords</li>
<li>Basics of SSH tunnels, and how they are used in PEcAn</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Description of PEcAn related tools that control remote execution</li>
</ol>
<ul>
<li>Basic remote execution R functions in <code>PEcAn.remote</code></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>SETUP- Configuration Files and settings</li>
</ol>
<ul>
<li>Remote model execution configuration in the <code>pecan.xml</code> and <code>config.php</code></li>
<li>Additional information about preparing remote servers for execution</li>
</ul>
</div>
<div id="basics-of-ssh" class="section level4">
<h4><span class="header-section-number">5.2.0.53</span> Basics of SSH</h4>
<p>All of the PEcAn remote infrastructure depends on the system <code>ssh</code> utility, so it’s important to make sure this works before attempting the advanced remote execution functionality in PEcAn.</p>
<p>To connect to a remote server interactively, the command is simply:</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> <span class="op">&lt;</span>username<span class="op">&gt;</span>@<span class="op">&lt;</span>hostname<span class="op">&gt;</span></code></pre>
<p>For instance, my connection to the BU shared computing cluster looks like:</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> ashiklom@geo.bu.edu</code></pre>
<p>It will prompt me for my BU password, and, if successful, will drop me into a login shell on the remote machine.</p>
<p>Alternatively to the login shell, <code>ssh</code> can be used to execute arbitrary code, whose output will be returned exactly as it would if you ran the command locally.
For example, the following:</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> ashiklom@geo.bu.edu pwd</code></pre>
<p>will run the <code>pwd</code> command, and return the path to my home directory on the BU SCC.
The more advanced example below will run some simple R code on the BU SCC and return the output as if it was run locally.</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> ashiklom@geo.bu.edu Rscript -e <span class="st">&quot;seq(1, 10)&quot;</span></code></pre>
</div>
<div id="ssh-authentication-password-vs.ssh-key" class="section level4">
<h4><span class="header-section-number">5.2.0.54</span> SSH authentication – password vs. SSH key</h4>
<p>Because this server uses passwords for authentication, this command will then prompt me for my password.</p>
<p>An alternative to password authentication is using SSH keys.
Under this system, the host machine (say, your laptop, or the PEcAn VM) has to generate a public and private key pair (using the <code>ssh-keygen</code> command).
The private key (by default, a file in <code>~/.ssh/id_rsa</code>) lives on the host machine, and should <strong>never</strong> be shared with anyone.
The public key will be distributed to any remote machines to which you want the host to be able to connect.
On each remote machine, the public key should be added to a list of authorized keys located in the <code>~/.ssh/authorized_keys</code> file (on the remote machine).
The authorized keys list indicates which machines (technically, which keys – a single machine, and even a single user, can have many keys) are allowed to connect to it.
This is the system used by all of the PEcAn servers (<code>pecan1</code>, <code>pecan2</code>, <code>test-pecan</code>).</p>
</div>
<div id="ssh-tunneling" class="section level4">
<h4><span class="header-section-number">5.2.0.55</span> SSH tunneling</h4>
<p>SSH authentication can be more advanced than indicated above, especially on systems that require dual authentication.
Even simple password-protection can be tricky in scripts, since (by design) it is fairly difficult to get SSH to accept a password from anything other than the raw keyboard input (i.e. SSH doesn’t let you pass passwords as input or arguments, because this exposes your password as plain text).</p>
<p>A convenient and secure way to follow SSH security protocol, but prevent having to go through the full authentication process every time, is to use SSH tunnels (or “sockets”, which are effectively synonymous).
Essentially, an SSH socket is a read- and write-protectected file that contains all of the information about an SSH connection.</p>
<p>To create an SSH tunnel, use a command like the following:</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> -n -N -f -o ControlMaster=yes -S /path/to/socket/file <span class="op">&lt;</span>username<span class="op">&gt;</span>@<span class="op">&lt;</span>hostname<span class="op">&gt;</span></code></pre>
<p>If appropriate, this will prompt you for your password (if using password authentication), and then will drop you back to the command line (thanks to the <code>-N</code> flag, which runs SSH without executing a command, the <code>-f</code> flag, which pushes SSH into the background, and the <code>-n</code> flag, which prevents ssh from reading any input).
It will also create the file <code>/path/to/socket/file</code>.</p>
<p>To use this socket with another command, use the <code>-S /path/to/file</code> flag, pointing to the same tunnel file you just created.</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> -S /path/to/socket/file <span class="op">&lt;</span>hostname<span class="op">&gt;</span> <span class="op">&lt;</span>optional command<span class="op">&gt;</span></code></pre>
<p>This will let you access the server without any sort of authentication step.
As before, if <code>&lt;optional command&gt;</code> is blank, you will be dropped into an interactive shell on the remote, or if it’s a command, that command will be executed and the output returned.</p>
<p>To close a socket, use the following:</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> -S /path/to/socket/file <span class="op">&lt;</span>hostname<span class="op">&gt;</span> -O exit</code></pre>
<p>This will delete the socket file and close the connection.
Alternatively, a scorched earth approach to closing the SSH tunnel if you don’t remember where you put the socket file is something like the following:</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="ex">pgrep</span> ssh   # See which processes will be killed
<span class="ex">pkill</span> ssh   # Kill those processes</code></pre>
<p>…which will kill all user processes called <code>ssh</code>.</p>
<p>To automatically create tunnels following a specific pattern, you can add the following to your
<code>~/.ssh/config</code></p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="ex">Host</span> <span class="op">&lt;</span>hostname goes here<span class="op">&gt;</span>
 <span class="ex">ControlMaster</span> auto
 <span class="ex">ControlPath</span> /tmp/%r@%h:%p</code></pre>
<p>For more information, see <code>man ssh</code>.</p>
</div>
<div id="ssh-tunnels-and-pecan" class="section level4">
<h4><span class="header-section-number">5.2.0.56</span> SSH tunnels and PEcAn</h4>
<p>Many of the <code>PEcAn.remote</code> functions assume that a tunnel is already open.
If working from the web interface, the tunnel will be opened for you by some under-the-hood PHP and Bash code, but if debugging or working locally, you will have to create the tunnel yourself.
The best way to do this is to create the tunnel first, outside of R, as described above.
(In the following examples, I’ll use my username <code>ashiklom</code> connecting to the <code>test-pecan</code> server with a socket stored in <code>/tmp/testpecan</code>.
To follow along, replace these with your own username and designated server, respectively).</p>
<pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">ssh</span> -nNf -o ControlMaster=yes -S /tmp/testpecan ashiklom@test-pecan.bu.edu</code></pre>
<p>Then, in R, create a <code>host</code> object, which is just a list containing the elements <code>name</code> (hostname) and <code>tunnel</code> (path to tunnel file).</p>
<pre class="sourceCode r"><code class="sourceCode r">my_host &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">name =</span> <span class="st">&quot;test-pecan.bu.edu&quot;</span>, <span class="dt">tunnel =</span> <span class="st">&quot;/tmp/testpecan&quot;</span>)</code></pre>
<p>This host object can then be used in any of the remote execution functions.</p>
</div>
<div id="basic-remote-execute-functions" class="section level4">
<h4><span class="header-section-number">5.2.0.57</span> Basic remote execute functions</h4>
<p>The <code>PEcAn.remote::remote.execute.cmd</code> function runs a system command on a remote server (or on the local server, if <code>host$name == &quot;localhost&quot;</code>).</p>
<pre><code>x &lt;- PEcAn.remote::remote.execute.cmd(host = my_host, cmd = &quot;echo&quot;, args = &quot;Hello world&quot;)
x</code></pre>
<p>Note that <code>remote.execute.cmd</code> is similar to base R’s <code>system2</code>, in that the base command (in this case, <code>echo</code>) is passed separately from its arguments (<code>&quot;Hello world&quot;</code>).
Note also that the output of the remote command is returned as a character.</p>
<p>For R code, there is a special wrapper around <code>remote.execute.cmd</code> – <code>PEcAn.remote::remote.execute.R</code>, which runs R code (passed as a string) on a remote and returns the output.</p>
<pre><code>code &lt;- &quot;
    x &lt;- 2:4
    y &lt;- 3:1
    x ^ y
&quot;
out &lt;- PEcAn.remote::remote.execute.R(code = code, host = my_host)</code></pre>
<p>For additional functions related to remote file operations and other stuff, see the <code>PEcAn.remote</code> package documentation.</p>
</div>
<div id="remote-model-execution-with-pecan" class="section level4">
<h4><span class="header-section-number">5.2.0.58</span> Remote model execution with PEcAn</h4>
<p>The workhorse of remote model execution is the <code>PEcAn.remote::start.model.runs</code> function, which distributes execution of each run in a list of runs (e.g. multiple runs in an ensemble) to the local machine or a remote based on the configuration in the PEcAn settings.</p>
<p>Broadly, there are three major types of model execution:</p>
<ul>
<li>Serialized (<code>PEcAn.remote::start_serial</code>) – This runs models one at a time, directly on the local machine or remote (i.e. same as calling the executables one at a time for each run).</li>
<li>Via a queue system, (<code>PEcAn.remote::start_qsub</code>) – This uses a queue management system, such as SGE (e.g. <code>qsub</code>, <code>qstat</code>) found on the BU SCC machines, to submit jobs.
For computationally intensive tasks, this is the recommended way to go.</li>
<li>Via a model launcher script (<code>PEcAn.remote::setup_modellauncher</code>) – This is a highly customizable approach where task submission is controlled by a user-provided script (<code>launcher.sh</code>).</li>
</ul>
</div>
<div id="xml-configuration" class="section level4">
<h4><span class="header-section-number">5.2.0.59</span> XML configuration</h4>
<p>The relevant section of the PEcAn XML file is the <code>&lt;host&gt;</code> block.
Here is a minimal example from one of my recent runs:</p>
<pre><code>&lt;host&gt;
    &lt;name&gt;geo.bu.edu&lt;/name&gt;
    &lt;user&gt;ashiklom&lt;/user&gt;
    &lt;tunnel&gt;/home/carya/output//PEcAn_99000000008/tunnel/tunnel&lt;/tunnel&gt;
&lt;/host&gt;</code></pre>
<p>Breaking this down:</p>
<ul>
<li><code>name</code> – The hostname of the machine where the runs will be performed.
Set it to <code>localhost</code> to run on the local machine.</li>
<li><code>user</code> – Your username on the remote machine (note that this may be different from the username on your local machine).</li>
<li><code>tunnel</code> – This is the tunnel file for the connection used by all remote execution files.
The tunnel is created automatically by the web interface, but must be created by the user for command line execution.</li>
</ul>
<p>This configuration will run in serialized mode.
To use <code>qsub</code>, the configuration is slightly more involved:</p>
<pre><code>&lt;host&gt;
  &lt;name&gt;geo.bu.edu&lt;/name&gt;
  &lt;user&gt;ashiklom&lt;/user&gt;
  &lt;qsub&gt;qsub -V -N @NAME@ -o @STDOUT@ -e @STDERR@ -S /bin/bash&lt;/qsub&gt;
  &lt;qsub.jobid&gt;Your job ([0-9]+) .*&lt;/qsub.jobid&gt;
  &lt;qstat&gt;qstat -j @JOBID@ || echo DONE&lt;/qstat&gt;
  &lt;tunnel&gt;/home/carya/output//PEcAn_99000000008/tunnel/tunnel&lt;/tunnel&gt;
&lt;/host&gt;</code></pre>
<p>The additional fields are as follows:</p>
<ul>
<li><code>qsub</code> – The command used to submit jobs to the queue system.
Despite the name, this can be any command used for any queue system.
The following variables are available to be set here:
<ul>
<li><code>@NAME@</code> – Job name to display</li>
<li><code>@STDOUT@</code> – File to which <code>stdout</code> will be redirected</li>
<li><code>@STDERR@</code> – File to which <code>stderr</code> will be redirected</li>
</ul></li>
<li><code>qsub.jobid</code> – A regular expression, from which the job ID will be determined.
This string will be parsed by R as <code>jobid &lt;- gsub(qsub.jobid, &quot;\\1&quot;, output)</code> – note that the first pattern match is taken as the job ID.</li>
<li><code>qstat</code> – The command used to check the status of a job.
Internally, PEcAn will look for the <code>DONE</code> string at the end, so a structure like <code>&lt;some command indicating if any jobs are still running&gt; || echo DONE</code> is required.
The <code>@JOBID@</code> here is the job ID determined from the <code>qsub.jobid</code> parsing.</li>
</ul>
<p>Documentation for using the model launcher is currently unavailable.</p>
</div>
<div id="configuration-for-pecan-web-interface" class="section level4">
<h4><span class="header-section-number">5.2.0.60</span> Configuration for PEcAn web interface</h4>
<p>The <code>config.php</code> has a few variables that will control where the web
interface can run jobs, and how to run those jobs. It is located in the <code>/web</code> directory and if you have not touched it yet it will
be named as <code>config.example.php</code>. Rename it to ’config.php` and edit by folowing the following directions.</p>
<p>These variables are <code>$hostlist</code>, <code>$qsublist</code>, <code>$qsuboptions</code>, and <code>$SSHtunnel</code>. In
the near future <code>$hostlist</code>, <code>$qsublist</code>, <code>$qsuboptions</code> will be
combined into a single list.</p>
<p><code>$SSHtunnel</code> : points to the script that creates an SSH tunnel.
The script is located in the web folder and the default value of
<code>dirname(__FILE__) . DIRECTORY_SEPARATOR . &quot;sshtunnel.sh&quot;;</code> most
likely will work.</p>
<p><code>$hostlist</code> : is an array with by default a single value, only
allowing jobs to run on the local server. Adding any other servers
to this list will show them in the pull down menu when selecting
machines, and will trigger the web page to be show to ask for a
username and password for the remote execution (make sure to use
HTTPS setup when asking for password to prevent it from being send
in the clear).</p>
<p><code>$qsublist</code> : is an array of hosts that require qsub to be used
when running the models. This list can include <code>$fqdn</code> to indicate
that jobs on the local machine should use qsub to run the models.</p>
<p><code>$qsuboptions</code> : is an array that lists options for each machine.
Currently it support the following options (see also
[Run Setup] and look at the tags)</p>
<pre><code>array(&quot;geo.bu.edu&quot; =&gt;
    array(&quot;qsub&quot;   =&gt; &quot;qsub -V -N @NAME@ -o @STDOUT@ -e @STDERR@ -S /bin/bash&quot;,
          &quot;jobid&quot;  =&gt; &quot;Your job ([0-9]+) .*&quot;,
          &quot;qstat&quot;  =&gt; &quot;qstat -j @JOBID@ || echo DONE&quot;,
          &quot;job.sh&quot; =&gt; &quot;module load udunits R/R-3.0.0_gnu-4.4.6&quot;,
          &quot;models&quot; =&gt; array(&quot;ED2&quot;    =&gt; &quot;module load hdf5&quot;))</code></pre>
<p>In this list <code>qsub</code> is the actual command line for qsub, <code>jobid</code>
is the text returned from qsub, <code>qstat</code> is the command to check
to see if the job is finished. <code>job.sh</code> and the value in models
are additional entries to add to the job.sh file generated to
run the model. This can be used to make sure modules are loaded
on the HPC cluster before running the actual model.</p>
</div>
<div id="running-pecan-code-for-remotely" class="section level4">
<h4><span class="header-section-number">5.2.0.61</span> Running PEcAn code for remotely</h4>
<p>You do not need to download PEcAn fully on your remote machine. You can compile and install the model specific code pieces of
PEcAn on the cluster easily without having to install the
full code base of PEcAn (and all OS dependencies). Use the <code>git clone</code> command to:</p>
<pre><code>devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;base/utils&#39;)</code></pre>
<p>Next we need to install the model specific pieces, this is done
almost the same (example for ED2):</p>
<pre><code>devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;models/ed&#39;)</code></pre>
<p>This should install dependencies required.</p>
<ul>
<li>The following are some notes on how to install the model specifics on different HPC
clusters*</li>
</ul>
</div>
<div id="geo.bu.edu" class="section level4">
<h4><span class="header-section-number">5.2.0.62</span> geo.bu.edu</h4>
<p>Following modules need to be loaded:</p>
<pre><code>module load hdf5 udunits R/R-3.0.0_gnu-4.4.6</code></pre>
<p>Next the following packages need to be installed, otherwise it
will fall back on the older versions install site-library</p>
<pre><code>install.packages(c(&#39;udunits2&#39;, &#39;lubridate&#39;), 
   configure.args=c(udunits2=&#39;--with-udunits2-lib=/project/earth/packages/udunits-2.1.24/lib --with-udunits2-include=/project/earth/packages/udunits-2.1.24/include&#39;),
   repos=&#39;http://cran.us.r-project.org&#39;)</code></pre>
<p>Finally to install support for both ED and SIPNET:</p>
<pre><code>devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;base/utils&#39;)
devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;models/sipnet&#39;)
devtools::install_github(&quot;pecanproject/pecan&quot;, subdir = &#39;models/ed&#39;)</code></pre>

</div>
</div>
<div id="advanced-user" class="section level2">
<h2><span class="header-section-number">5.3</span> Advanced User Guide</h2>
<ul>
<li><a href="user-section.html#adding-to-pecan">Adding to PEcAn as a user</a></li>
<li><a href="user-section.html#web-curl-submission">Workflow curl submission</a></li>
</ul>

<div id="adding-to-pecan" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Adding to PEcAn</h3>
<ul>
<li>Case studies
<ul>
<li><a href="user-section.html#adding-model">Adding a model</a></li>
<li><a href="user-section.html#NewInput">Adding input data</a></li>
<li><a href="user-section.html#adding-data-web">Adding data through the web interface</a></li>
<li>Adding new species, PFTs, and traits from a new site
<ul>
<li>Add a site</li>
<li>Add some species</li>
<li>Add PFT</li>
<li>Add trait data</li>
</ul></li>
<li>Adding a benchmark</li>
<li>Adding a met driver</li>
</ul></li>
<li><a href="user-section.html#editing-records">Reference</a> (How to edit records in bety)
<ul>
<li>Models</li>
<li>Species</li>
<li>PFTs</li>
<li>Traits</li>
<li>Inputs</li>
<li>DB files</li>
<li>Variables</li>
<li>Formats</li>
<li>(Link each section to relevant Bety tables)</li>
</ul></li>
</ul>
<div id="adding-model" class="section level4">
<h4><span class="header-section-number">5.3.1.1</span> Adding An Ecosystem Model</h4>
<p><strong>Adding a model to PEcAn involves two activities:</strong></p>
<ol style="list-style-type: decimal">
<li>Updating the PEcAn database to register the model</li>
<li>Writing the interface modules between the model and PEcAn</li>
</ol>
<p><strong>Note that coupling a model to PEcAn should not require any changes to the model code itself</strong>. A key aspect of our design philosophy is that we want it to be easy to add models to the system and we want to using the working version of the code that is used by all other model users, not a special branch (which would rapidly end up out-of-date).</p>
</div>
<div id="pecan-database" class="section level4">
<h4><span class="header-section-number">5.3.1.2</span> PEcAn Database</h4>
<p>To run a model within PEcAn requires that the PEcAn database know about the model – this includes a MODEL_TYPE designation, the types of inputs the model requires, the location of the model executable, and the plant functional types used by the model. The instructions below assume that you will be specifying this information using the BETYdb web-based interface. This can be done either on your local VM (localhost:3280/bety or localhost:6480/bety) or on a server installation of BETYdb, though in either case we’d encourage you to set up your PEcAn instance to support <a href="https://github.com/PecanProject/bety/wiki/Distributed-BETYdb">database syncs</a> so that these changes can be shared and backed-up across the PEcAn network.</p>
<p>The figure below summarizes the relevant database tables that need to be updated to add a new model and the primary variables that define each table.</p>
<p><img src="https://www.lucidchart.com/publicSegments/view/54a8aea8-9360-4628-af9e-392a0a00c27b/image.png" /></p>
</div>
<div id="define-model_type" class="section level4">
<h4><span class="header-section-number">5.3.1.3</span> Define MODEL_TYPE</h4>
<p>The first step to adding a model is to create a new MODEL_TYPE, which defines the abstract model class which we will then use to specify input requirements, define plant functional types, and keep track of different model versions. A MODEL_TYPE is created by selecting Runs &gt; Model Type and then clicking on <em>New Model Type</em>. The MODEL_TYPE name should be identical to the MODEL package name (see Interface Module below) and is case sensitive.</p>
</div>
<div id="machine" class="section level4">
<h4><span class="header-section-number">5.3.1.4</span> MACHINE</h4>
<p>The PEcAn design acknowledges that the same model executables and input files may exist on multiple computers. Therefore, we need to define the machine that that we are using. If you are running on the VM then the local machine is already defined as <em>pecan</em>. Otherwise, you will need to select Runs &gt; Machines, click <em>New Machine</em>, and enter the URL of your server (e.g. pecan2.bu.edu).</p>
</div>
<div id="model" class="section level4">
<h4><span class="header-section-number">5.3.1.5</span> MODEL</h4>
<p>Next we are going to tell PEcAn where the model executable is. Select Runs &gt; Files, and click ADD. Use the pull down menu to specify the machine you just defined above and fill in the path and name for the executable. For example, if SIPNET is installed at /usr/local/bin/sipnet then the path is /usr/local/bin/ and the file (executable) is sipnet.</p>
<p>Now we will create the model record and associate this with the File we just registered. The first time you do this select Runs &gt; Models and click <em>New Model</em>. Specify a descriptive name of the model (which doesn’t have to be the same as MODEL_TYPE), select the MODEL_TYPE from the pull down, and provide a revision identifier for the model (e.g. v3.2.1). Once the record is created select it from the Models table and click EDIT RECORD. Click on “View Related Files” and when the search window appears search for the model executable you just added (if you are unsure which file to choose you can go back to the Files menu and look up the unique ID number). You can then associate this Model record with the File by clicking on the +/- symbol. By contrast, clicking on the name itself will take you to the File record.</p>
<p>In the future, if you set up the SAME MODEL VERSION on a different computer you can add that Machine and File to PEcAn and then associate this new File with this same Model record. A single version of a model should only be entered into PEcAn <strong>once</strong>.</p>
<p>If a new version of the model is developed that is derived from the current version you should add this as a new Model record but with the same MODEL_TYPE as the original. Furthermore, you should set the previous version of the model as Parent of this new version.</p>
</div>
<div id="formats" class="section level4">
<h4><span class="header-section-number">5.3.1.6</span> FORMATS</h4>
<p>The PEcAn database keep track of all the input files passed to models, as well as any data used in model validation or data assimilation. Before we start to register these files with PEcAn we need to define the format these files will be in. To create a new format see <a href="user-section.html#NewFormat">Formats Documentation</a>.</p>
</div>
<div id="model_type---formats" class="section level4">
<h4><span class="header-section-number">5.3.1.7</span> MODEL_TYPE -&gt; Formats</h4>
<p>For each of the input formats you specify for your model, you will need to edit your MODEL_TYPE record to add an association between the format and the MODEL_TYPE. Go to Runs &gt; Model Type, select your record and click on the Edit button. Next, click on “Edit Associated Formats” and choose the Format you just defined from the pull down menu. If the <em>Input</em> box is checked then all matching Input records will be displayed in the PEcAn site run selection page when you are defining a model run. In other words, the set of model inputs available through the PEcAn web interface is model-specific and dynamically generated from the associations between MODEL_TYPEs and Formats. If you also check the <em>Required</em> box, then the Input will be treated as required and PEcAn will not run the model if that input is not available. Furthermore, on the site selection webpage, PEcAn will filter the available sites and only display pins on the Google Map for sites that have a full set of required inputs (or where those inputs could be generated using PEcAn’s workflows). Similarly, to make a site appear on the Google Map, all you need to do is specify Inputs, as described in the next section, and the point should automatically appear on the map.</p>
</div>
<div id="inputs" class="section level4">
<h4><span class="header-section-number">5.3.1.8</span> INPUTS</h4>
<p>After a file Format has been created then input files can be registered with the database. Creating Inputs can be found under <a href="user-section.html#NewInput">How to insert new Input data</a>.</p>
</div>
<div id="pfts-plant-functional-types" class="section level4">
<h4><span class="header-section-number">5.3.1.9</span> PFTS (Plant Functional Types)</h4>
<p>Since many of the PEcAn tools are designed to keep track of parameter uncertainties and assimilate data into models, to use PEcAn with a model it is important to define Plant Functional Types for the sites or regions that you will be running the model. PFTs are MODEL_TYPE specific, so when you create a new PFT entry (Data &gt; PFTs; New PFT) you will want to choose your MODEL_TYPE from the pull down and then give the PFT a descriptive name (e.g. temperate deciduous).</p>
</div>
<div id="species" class="section level4">
<h4><span class="header-section-number">5.3.1.10</span> Species</h4>
<p>Within PEcAn there are no predefined PFTs and user can create new PFTs very easily at whatever taxonomic level is most appropriate, from PFTs for individual species up to one PFT for all plants globally. To allow PEcAn to query its trait database for information about a PFT, you will want to associate species with the PFT record by choosing Edit and then “View Related Species”. Species can be searched for by common or scientific name and then added to a PFT using the +/- button.</p>
</div>
<div id="cultivars" class="section level4">
<h4><span class="header-section-number">5.3.1.11</span> Cultivars</h4>
<p>You can also define PFTs whose members are <em>cultivars</em> instead of species. This is designed for analyses where you want to want to perform meta-analysis on within-species comparisons (e.g. cultivar evaluation in an agricultural model) but may be useful for other cases when you want to specify different priors for some member of a species. You cannot associate both species and cultivars with the same PFT, but the cultivars in a cultivar PFT may come from different species, potentially including all known cultivars from some of the species, if you wish to and have thought about how to interpret the results.</p>
<p>It is not yet possible to add a cultivar PFT through the BETYdb web interface. See <a href="https://github.com/PecanProject/pecan/pull/1826#issuecomment-360665864">this GithHub comment</a> for an example of how to define one manually in PostgreSQL.</p>
</div>
<div id="priors" class="section level4">
<h4><span class="header-section-number">5.3.1.12</span> PRIORS</h4>
<p>In addition to adding species, a PFT is defined in PEcAn by the list of variables associated with the PFT. PEcAn takes a fundamentally Bayesian approach to representing model parameters, so variables are not entered as fixed constants but as Prior probability distributions (see below). Once Priors are defined for each model variable then you Edit the PFT and use “View Related Priors” to search for and add Prior distributions for each model parameter. It is important to note that the priors are defined for the variable name and units as specified in the Variables table. <strong>If the variable name or units is different within the model it is the responsibility of write.configs.MODEL function to handle name and unit conversions</strong> (see Interface Modules below). This can also include common but nonlinear transformations, such as converting SLA to LMA or changing the reference temperature for respiration rates.</p>
<p>There are a wide variety of priors already defined in the PEcAn database that often range from very diffuse and generic to very informative priors for specific PFTs. If the current set of Priors for a variable are inadequate, or if a prior needs to be specified for a new variable, this can be done under Data &gt; Priors then “New Prior”. After using the pull-down menu to select the Variable you want to generate a prior for, the prior is defined by choosing a probability distribution and specifying values for that distribution’s parameters. These are labeled Parameter a &amp; b but their exact meaning depends upon the distribution chosen. For example, for the Normal distribution a and b are the mean and standard deviation while for the Uniform they are the minimum and maximum. All parameters are defined based on their standard parameterization in the R language. If the prior is based on observed data (independent of data in the PEcAn database) then you can also specify the prior sample size, <em>N</em>. The <em>Phylogeny</em> variable allows one to specify what taxonomic grouping the prior is defined for, at it is important to note that this is just for reference and doesn’t have to be specified in any standard way nor does it have to be monophyletic (i.e. it can be a functional grouping). Finally, the <em>Citation</em> is a required variable that provides a reference for how the prior was defined. That said, there are a number of unpublished Citations in current use that simply state the expert opinion of an individual.</p>
<p>Additional information on adding PFTs, Species, and Priors can be found under [[Choosing PFTs]]</p>
</div>
<div id="interface-modules" class="section level4">
<h4><span class="header-section-number">5.3.1.13</span> Interface Modules</h4>
</div>
<div id="setting-up-the-module-directory-required" class="section level4">
<h4><span class="header-section-number">5.3.1.14</span> Setting up the module directory (required)</h4>
<p>PEcAn assumes that the interface modules are available as an R package in the models directory named after the model in question. The simplest way to get started on that R package is to make a copy the <a href="https://github.com/PecanProject/pecan/tree/master/models/template"><em>template</em></a> directory in the pecan/models folder and re-name it to the name of your model. In the code, filenames, and examples below you will want to substitute the word <strong>MODEL</strong> for the name of your model (note: R is case-sensitive).</p>
<p>If you do not want to write the interface modules in R then it is fairly simple to set up the R functions describe below to just call the script you want to run using R’s <em>system</em> command. Scripts that are not R functions should be placed in the <em>inst</em> folder and R can look up the location of these files using the function <em>system.file</em> which takes as arguments the <em>local</em> path of the file within the package folder and the name of the package (typically PEcAn.MODEL). For example</p>
<pre><code>## Example met conversion wrapper function
met2model.MODEL &lt;- function(in.path, in.prefix, outfolder, start_date, end_date){
   myMetScript &lt;- system.file(&quot;inst/met2model.MODEL.sh&quot;, &quot;PEcAn.MODEL&quot;)
   system(paste(myMetScript, file.path(in.path, in.prefix), outfolder, start_date, end_date))
}</code></pre>
<p>would execute the following at the Linux command line</p>
<pre><code>inst/met2model.MODEL.sh in.path/in.prefix outfolder start_date end_date    `</code></pre>
</div>
<div id="description" class="section level4">
<h4><span class="header-section-number">5.3.1.15</span> DESCRIPTION</h4>
<p>Within the module folder open the <em>DESCRIPTION</em> file and change the package name to PEcAn.MODEL. Fill out other fields such as Title, Author, Maintainer, and Date.</p>
</div>
<div id="namespace" class="section level4">
<h4><span class="header-section-number">5.3.1.16</span> NAMESPACE</h4>
<p>Open the <em>NAMESPACE</em> file and change all instances of MODEL to the name of your model. If you are not going to implement one of the optional modules (described below) at this time then you will want to comment those out using the pound sign <code>#</code>. For a complete description of R NAMESPACE files <a href="http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#Package-namespaces">see here</a>. If you create additional functions in your R package that you want to be used make sure you include them in the NAMESPACE as well (internal functions don’t need to be declared)</p>
</div>
<div id="building-the-package" class="section level4">
<h4><span class="header-section-number">5.3.1.17</span> Building the package</h4>
<p>Once the package is defined you will then need to add it to the PEcAn build scripts. From the root of the pecan directory, go into the <em>scripts</em> folder and open the file <em>build.sh</em>. Within the section of code that includes PACKAGES= add model/MODEL to the list of packages to compile. If, in writing your module, you add any other R packages to the system you will want to make sure those are listed in the DESCRIPTION and in the script <strong>scripts/install.dependencies.R</strong>. Next, from the root pecan directory open all/DESCRIPTION and add your model package to the <em>Suggests:</em> list.</p>
<p>At any point, if you want to check if PEcAn can build your MODEL package successfully, just go to the linux command prompt and run <strong>scripts/build.sh</strong>. You will need to do this before the system can use these packages.</p>
</div>
<div id="write.config.model-required" class="section level4">
<h4><span class="header-section-number">5.3.1.18</span> write.config.MODEL (required)</h4>
<p>This module performs two primary tasks. The first is to take the list of parameter values and model input files that it receives as inputs and write those out in whatever format(s) the MODEL reads (e.g. a settings file). The second is to write out a shell script, jobs.sh, which, when run, will start your model run and convert its output to the PEcAn standard (netCDF with metadata currently equivalent to the <a href="http://nacp.ornl.gov/MsTMIP_variables.shtml">MsTMIP standard</a>). Within the MODEL directory take a close look at inst/template.job and the example write.config.MODEL to see an example of how this is done. It is important that this script writes or moves outputs to the correct location so that PEcAn can find them. The example function also shows an example of writing a model-specific settings/config file, also by using a template.</p>
<p>You are encouraged to read the section above on defining PFTs before writing write.config.MODEL so that you understand what model parameters PEcAn will be passing you, how they will be named, and what units they will be in. Also note that the (optional) PEcAn input/driver processing scripts are called by separate workflows, so the paths to any required inputs (e.g. meteorology) will already be in the model-specific format by the time write.config.MODEL receives that info.</p>
</div>
<div id="output-conversions" class="section level4">
<h4><span class="header-section-number">5.3.1.19</span> Output Conversions</h4>
<p>The module model2netcdf.MODEL converts model output into the PEcAn standard (netCDF with metadata currently equivalent to the <a href="http://nacp.ornl.gov/MsTMIP_variables.shtml">MsTMIP standard</a>). This function was previously required, but now that the conversion is called within jobs.sh it may be easier for you to convert outputs using other approaches (or to just directly write outputs in the standard).</p>
<p>Whether you implement this function or convert outputs some other way, please note that PEcAn expects all outputs to be broken up into ANNUAL files with the year number as the file name (i.e. YEAR.nc), though these files may contain any number of scalars, vectors, matrices, or arrays of model outputs, such as time-series of each output variable at the model’s native timestep.</p>
<p>Note: PEcAn reads all variable names from the files themselves so it is possible to add additional variables that are not part of the MsTMIP standard. Similarly, there are no REQUIRED output variables, though <em>time</em> is highly encouraged. We are shortly going establish a canonical list of PEcAn variables so that if users add additional output variables they become part of the standard. <strong>We don’t want two different models to call the same output with two different names or different units</strong> as this would prohibit the multi-model syntheses and comparisons that PEcAn is designed to facilitate.</p>
</div>
<div id="met2model.model" class="section level4">
<h4><span class="header-section-number">5.3.1.20</span> met2model.MODEL</h4>
<p><code>met2model.MODEL(in.path, in.prefix, outfolder, start_date, end_date)</code></p>
<p>Converts meteorology input files from the PEcAn standard (netCDF, CF metadata) to the format required by the model. This file is optional if you want to load all of your met files into the Inputs table as described in <a href="../developers_guide/How-to-insert-new-Input-data.html">How to insert new Input data</a>, which is often the easiest way to get up and running quickly. However, this function is required if you want to benefit from PEcAn’s meteorology workflows and model run cloning. You’ll want to take a close look at [Adding-an-Input-Converter] to see the exact variable names and units that PEcAn will be providing. Also note that PEcAn splits all meteorology up into ANNUAL files, with the year number explicitly included in the file name, and thus what PEcAn will actually be providing is <strong>in.path</strong>, the input path to the folder where multiple met files may stored, and <strong>in.prefix</strong>, the start of the filename that precedes the year (i.e. an individual file will be named <code>&lt;in.prefix&gt;.YEAR.nc</code>). It is valid for in.prefix to be blank. The additional REQUIRED arguments to met2model.MODEL are <strong>outfolder</strong>, the output folder where PEcAn wants you to write your meteorology, and <strong>start_date</strong> and <strong>end_date</strong>, the time range the user has asked the meteorology to be processed for.</p>
</div>
<div id="commit-changes" class="section level4">
<h4><span class="header-section-number">5.3.1.21</span> Commit changes</h4>
<p>Once the MODEL modules are written, you should follow the <a href="Using-Git.md">Using-Git</a> instructions on how to commit your changes to your local git repository, verify that PEcAn compiles using <em>scripts/build.sh</em>, push these changes to Github, and submit a pull request so that your model module is added to the PEcAn system. It is important to note that while we encourage users to make their models open, adding the PEcAn interface module to the Github repository in no way requires that the model code itself be made public. It does, however, allow anyone who already has a copy of the model code to use PEcAn so we strongly encourage that any new model modules be committed to Github.</p>
</div>
<div id="NewInput" class="section level4">
<h4><span class="header-section-number">5.3.1.22</span> Adding input data</h4>
</div>
<div id="input-records-in-bety" class="section level4">
<h4><span class="header-section-number">5.3.1.23</span> Input records in BETY</h4>
<p>All model input data or data used for model calibration/validation must be registered in the BETY database.</p>
<p>Before creating a new Input record, you must make sure that the format type of your data is registered in the database. If you need to make a new format record, see <a href="user-section.html#NewFormat">Creating a new format record in BETY</a>.</p>
</div>
<div id="create-a-database-file-record-for-the-input-data" class="section level4">
<h4><span class="header-section-number">5.3.1.24</span> Create a database file record for the input data</h4>
<p>An input record contains all the metadata required to identify the data, however, this record does not include the location of the data file. Since the same data may be stored in multiple places, every file has its own dbfile record.</p>
<p>From your BETY interface:</p>
<ul>
<li>Create a DBFILES entry for the path to the file
<ul>
<li>From the menu click RUNS then FILES</li>
<li>Click “New File”</li>
<li>Select the machine your file is located at</li>
<li>Fill in the File Path where your file is located (aka folder or directory) NOT including the name of the file itself</li>
<li>Fill in the File Name with the name of the file itself. Note that some types of input records will refer to be ALL the files in a directory and thus File Name can be blank</li>
<li>Click Update</li>
</ul></li>
</ul>
</div>
<div id="creating-a-new-input-record-in-bety" class="section level4">
<h4><span class="header-section-number">5.3.1.25</span> Creating a new Input record in BETY</h4>
<p>From your BETY interface:</p>
<ul>
<li>Create an INPUT entry for your data
<ul>
<li>From the menu click RUNS then INPUTS</li>
<li>Click “New Input”</li>
<li>Select the SITE that this data is associated with the input data set</li>
<li>Other required fields are a unique name for the input, the start and end dates of the data set, and the format of the data. If the data is not in a currently known format you will need to create a NEW FORMAT and possibly a new input converter. Instructions on how to do add a converter can be found here <a href="user-section.html#InputConversions">Input conversion</a>. Instructions on how to add a format record can be found <a href="user-section.html#NewFormat">here</a></li>
<li>Parent ID is an optional variable to indicated that one dataset was derived from another.</li>
<li>Click “Create”</li>
</ul></li>
<li>Associate the DBFILE with the INPUT
<ul>
<li>In the RUNS -&gt; INPUTS table, search and find the input record you just created</li>
<li>Click on the EDIT icon</li>
<li>Select “View related Files”</li>
<li>In the Search window, search for the DBFILE you just created</li>
</ul></li>
<li>Once you have found the DBFILE, click on the “+” icon to add the file</li>
<li>Click on “Update” at the bottom when you are done.</li>
</ul>
</div>
<div id="InputConversions" class="section level4">
<h4><span class="header-section-number">5.3.1.26</span> Adding a new input converter</h4>
<p>Three Types of data conversions are discussed below: Meteorological data, Vegetation data, and Soil data. Each section provides instructions on how to convert data from their raw formats into a PEcAn standard format, whether it be from a database or if you have raw data in hand.</p>
<p>Also, see <a href="pecan-standards.html#pecan-standards">PEcAn standard formats</a>.</p>
</div>
<div id="meterological-data" class="section level4">
<h4><span class="header-section-number">5.3.1.27</span> Meterological Data</h4>
<div id="adding-a-function-to-pecan-to-convert-a-met-data-source" class="section level5">
<h5><span class="header-section-number">5.3.1.27.1</span> Adding a function to PEcAn to convert a met data source</h5>
<p>In general, you will need to write a function to download the raw met data and one to convert it to the PEcAn standard.</p>
<p>Downloading raw data function are named <code>download.&lt;source&gt;.R</code>. These functions are stored within the PEcAn directory: <a href="https://github.com/PecanProject/pecan/tree/develop/modules/data.atmosphere/R"><code>/modules/data.atmosphere/R</code></a>.</p>
<p>Conversion function from raw to standard are named <code>met2CF.&lt;source&gt;.R</code>. These functions are stored within the PEcAn directory: <a href="https://github.com/PecanProject/pecan/tree/develop/modules/data.atmosphere/R"><code>/modules/data.atmosphere/R</code></a>.</p>
<p>Current Meteorological products that are coupled to PEcAn can be found in our <a href="available-meteorological-drivers.html#available-meteorological-drivers">Available Meteorological Drivers</a> page.</p>
<p>Note: Unless you are also adding a new model, you will not need to write a script to convert from PEcAn standard to PEcAn models. Those conversion scripts are written when a model is added and can be found within each model’s PEcAn directory.</p>
<p><em>Standards dimesion, names, nad units can be found here:</em> <a href="pecan-standards.html#input-standards">Input Standards</a></p>
</div>
<div id="adding-single-site-specific-meteorological-data" class="section level5">
<h5><span class="header-section-number">5.3.1.27.2</span> Adding Single-Site Specific Meteorological Data</h5>
<p>Perhaps you have meteorological data specific to one site, with a unique format that you would like to add to PEcAn. Your steps would be to:
1. write a script or function to convert your files into the netcdf PEcAn standard
2. insert that file as an input record for your site following these <a href="user-section.html#NewInput">instructions</a></p>
</div>
<div id="processing-met-data-outside-of-the-workflow-using-pecan-functions" class="section level5">
<h5><span class="header-section-number">5.3.1.27.3</span> Processing Met data outside of the workflow using PEcAn functions</h5>
<p>Perhaps you would like to obtain data from one of the sources coupled to PEcAn on its own. To do so you can run PEcAn functions on their own.</p>
<div id="example-1-processing-data-from-a-database" class="section level6">
<h6><span class="header-section-number">5.3.1.27.3.1</span> Example 1: Processing data from a database</h6>
<p>Download Amerifluxlbl from Niwot Ridge for the year 2004:</p>
<pre><code>raw.file &lt;-PEcAn.data.atmosphere::download.AmerifluxLBL(sitename = &quot;US-NR1&quot;, 
                                             outfolder = &quot;.&quot;, 
                                             start_date = &quot;2004-01-01&quot;, 
                                             end_date = &quot;2004-12-31&quot;)</code></pre>
<p>Using the information returned as the object <code>raw.file</code> you will then convert the raw files into a standard file.</p>
<p>Open a connection with BETY. You may need to change the host name depending on what machine you are hosting BETY. You can find the hostname listed in the machines table of BETY.</p>
<pre><code>
bety &lt;- dplyr::src_postgres(dbname   = &#39;bety&#39;, 
                            host =&#39;localhost&#39;, 
                            user     = &quot;bety&quot;, 
                            password = &quot;bety&quot;)
                            
con &lt;- bety$con</code></pre>
<p>Next you will set up the arguments for the function</p>
<pre><code>in.path &lt;- &#39;.&#39;
in.prefix &lt;- raw.file$dbfile.name
outfolder &lt;- &#39;.&#39;
format.id &lt;- 5000000002
format &lt;- PEcAn.DB::query.format.vars(format.id=format.id,bety = bety)
lon &lt;- -105.54
lat &lt;- 40.03
format$time_zone &lt;- &quot;America/Chicago&quot;</code></pre>
<p>Note: The format.id can be pulled from the BETY database if you know the format of the raw data.</p>
<p>Once these arguments are defined you can execute the <code>met2CF.csv</code> function</p>
<pre><code>PEcAn.data.atmosphere::met2CF.csv(in.path = in.path, 
                                  in.prefix =in.prefix,
                                  outfolder = &quot;.&quot;, 
                                  start_date =&quot;2004-01-01&quot;,
                                  end_date = &quot;2004-12-01&quot;,
                                  lat= lat,
                                  lon = lon,
                                  format = format) </code></pre>
</div>
<div id="example-2-processing-data-from-data-already-in-hand" class="section level6">
<h6><span class="header-section-number">5.3.1.27.3.2</span> Example 2: Processing data from data already in hand</h6>
<p>If you have Met data already in hand and you would like to convert into the PEcAn standard follow these instructions.</p>
<p>Update BETY with file record, format record and input record according to this page <a href="user-section.html#NewInput">How to Insert new Input Data</a></p>
<p>If your data is in a csv format you can use the <code>met2CF.csv</code>function to convert your data into a PEcAn standard file.</p>
<p>Open a connection with BETY. You may need to change the host name depending on what machine you are hosting BETY. You can find the hostname listed in the machines table of BETY.</p>
<pre><code>bety &lt;- dplyr::src_postgres(dbname   = &#39;bety&#39;, 
                            host =&#39;localhost&#39;, 
                            user     = &quot;bety&quot;, 
                            password = &quot;bety&quot;)
                            
con &lt;- bety$con</code></pre>
<p>Prepare the arguments you need to execute the met2CF.csv function</p>
<pre><code>in.path &lt;- &#39;path/where/the/raw/file/lives&#39;
in.prefix &lt;- &#39;prefix_of_the_raw_file&#39;
outfolder &lt;- &#39;path/to/where/you/want/to/output/thecsv/&#39;
format.id &lt;- formatid of the format your created
format &lt;- PEcAn.DB::query.format.vars(format.id=format.id,bety = bety)
lon &lt;- longitude of your site
lat &lt;- latitude of your site
format$time_zone &lt;- time zone of your site
start_date &lt;- Start date of your data in &quot;y-m-d&quot;
end_date &lt;- End date of your data in &quot;y-m-d&quot;</code></pre>
<p>Next you can execute the function:</p>
<pre><code>PEcAn.data.atmosphere::met2CF.csv(in.path = in.path, 
                                  in.prefix =in.prefix, 
                                  outfolder = &quot;.&quot;, 
                                  start_date = start_date,
                                  end_date = end_date,
                                  lat= lat,
                                  lon = lon,
                                  format = format)</code></pre>
</div>
</div>
</div>
<div id="vegetation-data" class="section level4">
<h4><span class="header-section-number">5.3.1.28</span> Vegetation Data</h4>
<p>Vegetation data will be required to parameterize your model. In these examples we will go over how to produce a standard initial condition file.</p>
<p>The main function to process cohort data is the <code>ic.process.R</code> function. As of now however, if you require pool data you will run a separate function, <code>pool_ic_list2netcdf.R</code>.</p>
<div id="example-1-processing-veg-data-from-data-in-hand." class="section level6">
<h6><span class="header-section-number">5.3.1.28.0.1</span> Example 1: Processing Veg data from data in hand.</h6>
<p>In the following example we will process vegetation data that you have in hand using PEcAn.</p>
<p>First, you’ll need to create a input record in BETY that will have a file record and format record reflecting the location and format of your file. Instructions can be found in our <a href="user-section.html#NewInput">How to Insert new Input Data</a> page.</p>
<p>Once you have created an input record you must take note of the input id of your record. An easy way to take note of this is in the URL of the BETY webpage that shows your input record. In this example we use an input record with the id <code>1000013064</code> which can be found at this url: <a href="https://psql-pecan.bu.edu/bety/inputs/1000013064#" class="uri">https://psql-pecan.bu.edu/bety/inputs/1000013064#</a> . Note that this is the Boston University BETY database. If you are on a different machine, your url will be different.</p>
<p>With the input id in hand you can now edit a pecan XML so that the PEcAn function <code>ic.process</code> will know where to look in order to process your data. The <code>inputs</code> section of your pecan XML will look like this. As of now ic.process is set up to work with the ED2 model so we will use ED2 settings and then grab the intermediary Rds data file that is created as the standard PEcAn file. For your Inputs section you will need to input your input id wherever you see the <code>useic</code> flag.</p>
<pre><code>&lt;inputs&gt;
      &lt;css&gt;
        &lt;source&gt;FFT&lt;/source&gt;
        &lt;output&gt;css&lt;/output&gt;
        &lt;username&gt;pecan&lt;/username&gt;
        &lt;id&gt;1000013064&lt;/id&gt;
        &lt;useic&gt;TRUE&lt;/useic&gt;
        &lt;metadata&gt;
          &lt;trk&gt;1&lt;/trk&gt;
          &lt;age&gt;70&lt;/age&gt;
          &lt;area&gt;400&lt;/area&gt;
        &lt;/metadata&gt;
      &lt;/css&gt;
      &lt;pss&gt;
        &lt;source&gt;FFT&lt;/source&gt;
        &lt;output&gt;pss&lt;/output&gt;
        &lt;username&gt;pecan&lt;/username&gt;
        &lt;id&gt;1000013064&lt;/id&gt;
        &lt;useic&gt;TRUE&lt;/useic&gt;
      &lt;/pss&gt;
      &lt;site&gt;
        &lt;source&gt;FFT&lt;/source&gt;
        &lt;output&gt;site&lt;/output&gt;
        &lt;username&gt;pecan&lt;/username&gt;
        &lt;id&gt;1000013064&lt;/id&gt;
        &lt;useic&gt;TRUE&lt;/useic&gt;
      &lt;/site&gt;
      &lt;met&gt;
        &lt;source&gt;CRUNCEP&lt;/source&gt;
        &lt;output&gt;ED2&lt;/output&gt;
      &lt;/met&gt;
      &lt;lu&gt;
        &lt;id&gt;294&lt;/id&gt;
      &lt;/lu&gt;
      &lt;soil&gt;
        &lt;id&gt;297&lt;/id&gt;
      &lt;/soil&gt;
      &lt;thsum&gt;
        &lt;id&gt;295&lt;/id&gt;
      &lt;/thsum&gt;
      &lt;veg&gt;
        &lt;id&gt;296&lt;/id&gt;
      &lt;/veg&gt;
    &lt;/inputs&gt;</code></pre>
<p>This IC workflow also supports generating ensembles of initial conditions from posterior estimates of DBH. To do this the tags below can be inserted to the pecan.xml:</p>
<pre><code>       &lt;css&gt;
        &lt;source&gt;PalEON&lt;/source&gt;
        &lt;output&gt;css&lt;/output&gt;
        &lt;id&gt;1000015682&lt;/id&gt;
        &lt;useic&gt;TRUE&lt;/useic&gt;
        &lt;ensemble&gt;20&lt;/ensemble&gt;
        &lt;metadata&gt;
          &lt;area&gt;1256.637&lt;/area&gt;
          &lt;n.patch&gt;3&lt;/n.patch&gt;
        &lt;/metadata&gt;
      &lt;/css&gt;</code></pre>
<p>Here the <code>id</code> should point to a file that has MCMC samples to generate the ensemble from. The number between the <code>&lt;ensemble&gt;</code> tag defines the number of ensembles requested. The workflow will populate the settings list <code>run$inputs</code> tag with ensemble member information. E.g.:</p>
<pre><code>  &lt;inputs&gt;
   &lt;css&gt;
    &lt;path1&gt;...&lt;/path1&gt;
    &lt;path2&gt;...&lt;/path2&gt;
    &lt;path3&gt;...&lt;/path3&gt;
    ...
    &lt;pathN&gt;...&lt;/pathN&gt;
   &lt;/css&gt;
   &lt;pss&gt;
    &lt;path&gt;
     &lt;path1&gt;...&lt;/path1&gt;
     &lt;path2&gt;...&lt;/path2&gt;
     &lt;path3&gt;...&lt;/path3&gt;
      ...
     &lt;pathN&gt;...&lt;/pathN&gt;
    &lt;/path&gt;
   &lt;/pss&gt;
   &lt;site&gt;
    &lt;path&gt;
     &lt;path1&gt;...&lt;/path1&gt;
     &lt;path2&gt;...&lt;/path2&gt;
     &lt;path3&gt;...&lt;/path3&gt;
      ...
     &lt;pathN&gt;...&lt;/pathN&gt;
    &lt;/path&gt;
   &lt;/site&gt;
   &lt;met&gt;...&lt;/met&gt;
   &lt;lu&gt;...&lt;/lu&gt;
   &lt;soil&gt;...&lt;/soil&gt;
   &lt;thsum&gt;...&lt;/thsum&gt;
   &lt;veg&gt;...&lt;/veg&gt;
  &lt;/inputs&gt;</code></pre>
<p>Once you edit your PEcAn.xml you can than create a settings object using PEcAn functions. Your <code>pecan.xml</code> must be in your working directory.</p>
<pre><code>settings &lt;- PEcAn.settings::read.settings(&quot;pecan.xml&quot;)
settings &lt;- PEcAn.settings::prepare.settings(settings, force=FALSE)</code></pre>
<p>You can then execute the <code>ic.process</code> function to convert data into a standard Rds file:</p>
<pre><code>input &lt;- settings$run$inputs
dir &lt;- &quot;.&quot;
ic.process(settings, input, dir, overwrite = FALSE)</code></pre>
<p>Note that the argument <code>dir</code> is set to the current directory. You will find the final ED2 file there. More importantly though you will find the <code>.Rds</code> file within the same directory.</p>
</div>
<div id="example-3-pool-initial-condition-files" class="section level6">
<h6><span class="header-section-number">5.3.1.28.0.2</span> Example 3 Pool Initial Condition files</h6>
<p>If you have pool vegetation data, you’ll need the <a href="https://github.com/PecanProject/pecan/blob/develop/modules/data.land/R/pool_ic_list2netcdf.R"><code>pool_ic_list2netcdf.R</code></a> function to convert the pool data into PEcAn
standard.</p>
<p>The function stands alone and requires that you provide a named list of netcdf dimensions and values, and a named list of variables and values. Names and units need to match the standard_vars.csv table found <a href="https://github.com/PecanProject/pecan/blob/develop/base/utils/data/standard_vars.csv">here</a>.</p>
<pre><code>#Create a list object with necessary dimensions for your site
input&lt;-list()
dims&lt;- list(lat=-115,lon=45, time= 1)
variables&lt;- list(SoilResp=8,TotLivBiom=295)
input$dims &lt;- dims
input$vals &lt;- variables</code></pre>
<p>Once this is done, set <code>outdir</code> to where you’d like the file to write out to and a siteid. Siteid in this can be used as an file name identifier. Once part of the automated workflow siteid will reflect the site id within the BET db.</p>
<pre><code>outdir  &lt;- &quot;.&quot;
siteid &lt;- 772
pool_ic_list2netcdf(input = input, outdir = outdir, siteid = siteid)</code></pre>
<p>You should now have a netcdf file with initial conditions.</p>
</div>
</div>
<div id="soil-data" class="section level4">
<h4><span class="header-section-number">5.3.1.29</span> Soil Data</h4>
<div id="example-1-converting-data-in-hand" class="section level6">
<h6><span class="header-section-number">5.3.1.29.0.1</span> Example 1: Converting Data in hand</h6>
<p>Local data that has the correct names and units can easily be written out in PEcAn standard using the function soil2netcdf.</p>
<pre><code>soil.data &lt;- list(volume_fraction_of_sand_in_soil = c(0.3,0.4,0.5),
                  volume_fraction_of_clay_in_soil = c(0.3,0.3,0.3),
                  soil_depth = c(0.2,0.5,1.0))
                         
soil2netcdf(soil.data,&quot;soil.nc&quot;)</code></pre>
<p>At the moment this file would need to be inserted into Inputs manually. By default, this function also calls soil_params, which will estimate a number of hydraulic and thermal parameters from texture. Be aware that at the moment not all model couplers are yet set up to read this file and/or convert it to model-specific formats.</p>
</div>
<div id="example-2-converting-paleon-data" class="section level6">
<h6><span class="header-section-number">5.3.1.29.0.2</span> Example 2: Converting PalEON data</h6>
<p>In addition to location-specific soil data, PEcAn can extract soil texture information from the PalEON regional soil product, which itself is a subset of the MsTMIP Unified North American Soil Map. If this product is installed on your machine, the appropriate step in the do_conversions workflow is enabled by adding the following tag under <code>&lt;inputs&gt;</code> in your pecan.xml</p>
<pre class="sourceCode xml"><code class="sourceCode xml">   <span class="kw">&lt;soil&gt;</span>
     <span class="kw">&lt;id&gt;</span>1000012896<span class="kw">&lt;/id&gt;</span>
   <span class="kw">&lt;/soil&gt;</span></code></pre>
<p>In the future we aim to extend this extraction to a wider range of soil products.</p>
</div>
<div id="example-3-extracting-soil-properties-from-gssurgo-database" class="section level6">
<h6><span class="header-section-number">5.3.1.29.0.3</span> Example 3: Extracting soil properties from gSSURGO database</h6>
<p>In addition to location-specific soil data, PEcAn can extract soil texture information from the gSSURGO data product. This product needs no installation and it extract soil proeprties for the lower 48 states in U.S. In order to let the pecan know that you’re planning to use gSSURGO, you can the following XML tag under input in your pecan xml file.</p>
<pre class="sourceCode xml"><code class="sourceCode xml"><span class="kw">&lt;inputs&gt;</span>
   <span class="kw">&lt;soil&gt;</span>
     <span class="kw">&lt;source&gt;</span>gSSURGO<span class="kw">&lt;/source&gt;</span>
   <span class="kw">&lt;/soil&gt;</span>
<span class="kw">&lt;/inputs&gt;</span></code></pre>
</div>
</div>
<div id="adding-data-web" class="section level4">
<h4><span class="header-section-number">5.3.1.30</span> Pecan Data Ingest via Web Interface</h4>
<p>This tutorial explains the process of ingesting data into PEcAn via our Data-Ingest Application. In order to ingest data, the users must first select data that they wish to upload. Then, they enter metadata to help PEcAn parse and load the data into the main PEcAn workflow.</p>
</div>
<div id="loading-data" class="section level4">
<h4><span class="header-section-number">5.3.1.31</span> Loading Data</h4>
</div>
<div id="selecting-ingest-method" class="section level4">
<h4><span class="header-section-number">5.3.1.32</span> Selecting Ingest Method</h4>
<p>The Data-Ingest application is capable of loading data from the DataONE data federation and from the user’s local machine. The first step in the workflow is therefore to select an upload method. The application defaults to uploading from DataONE. To upload data from a local device, simply select the radio button titled <code>Local Files</code>.</p>
</div>
<div id="dataone-upload-example" class="section level4">
<h4><span class="header-section-number">5.3.1.33</span> DataONE Upload Example</h4>
<p><br></p>
<p><img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/D1Ingest-1.gif" width="50%" height="50%" style="display: block; margin: auto;" />
<br>
The DataONE download feature allows the user to download data at a given doi or DataONE specific package id. To do so, enter the doi or identifier in the <code>Import From DataONE</code> field and select <code>download</code>. The download process may take a couple of minutes to run depending on the number of files in the dataONE package. This may be a convenient option if the user does not wish to download files directly to their local machine. Once the files have been successfully downloaded from DataONE, they are displayed in a table. Before proceeding to the next step, the user can select a file to ingest by clicking on the corresponding row in the data table.
<br></p>
</div>
<div id="local-upload-example" class="section level4">
<h4><span class="header-section-number">5.3.1.34</span> Local Upload Example</h4>
<p><br>
To upload local files, the user should first select the <code>Local Files</code> button. From there, the user can upload files from their local machines by selecting <code>Browse</code> or by dragging and dropping files into the text box. The files will begin uploading automatically. From there, the user should select a file to ingest and then select the <code>Next Step</code> button.
<br>
After this step, the workflow is identical for both methods. However, please note that if it becomes necessary to switch from loading data via <code>DataONE</code> to uploading local files after the first step, please restart the application.
<br></p>
<p><img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/Local_loader_sm.gif" width="50%" height="50%" style="display: block; margin: auto;" />
<br>
<img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/local_browse.gif" width="50%" height="50%" style="display: block; margin: auto;" />
#### 2. Creating an Input Record
Creating an input record requires some basic metadata about the file that is being ingested. Each entry field is briefly explained below.
<br></p>
<ul>
<li>Site: To link the selected file with a site, the user can scroll or type to search all the sites in PEcAn. See Example:
<br>
<img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/Selectize_Input_sm.gif" width="50%" height="50%" style="display: block; margin: auto;" />
<br></li>
<li><p>Parent: To link the selected file with another dataset, type to search existing datasets in the <code>Parent</code> field.</p></li>
<li><p>Name: this field should be autofilled by selecting a file in step 1.</p></li>
<li><p>Format: If the selected file has an existing format name, the user can search and select in the <code>Format</code> field. If the selected file’s format is not already in pecan, the user can create a new format by selecting <code>Create New Format</code>. Once this new format is created, it will automatically populate the <code>Format</code> box and the <code>Current Mimetype</code> box (See Section 3).</p></li>
<li><p>Mimetype: If the format already exists, select an existing mimetype.</p></li>
<li><p>Start and End Date and Time: Inputs can be entered manually or by using the user interface. See example</p></li>
</ul>
<p><br>
<img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/DateTime.gif" width="50%" height="50%" style="display: block; margin: auto;" /></p>
<ul>
<li>Notes: Describe the data that is being uploaded. Please include any citations or references.</li>
</ul>
</div>
<div id="creating-a-format-record" class="section level4">
<h4><span class="header-section-number">5.3.1.35</span> 3. Creating a format record</h4>
<p>If it is necessary to add a new format to PEcAn, the user should fill out the form attached to the <code>Create New Format</code> button. The inputs to this form are described below:</p>
<ul>
<li><p>Mimetype: type to search existing mimetypes. If the mimetype is not in that list, please click on the link <code>Create New Mimetype</code> and create a new mimetype via the BETY website.</p></li>
<li><p>New Format Name: Add the name of the new format. Please exclude spaces from the name. Instead please use underscores &quot;_&quot;.</p></li>
<li><p>Header: If there is space before the first line of data in the dataset, please select <code>Yes</code></p></li>
<li><p>Skip: The number of lines in the header that should be skipped before the data.</p></li>
<li><p>Please enter notes that describe the format.</p></li>
</ul>
<p>Example:
<br>
<img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/new_format_record.gif" width="50%" height="50%" style="display: block; margin: auto;" />
#### 4. Formats_Variables Record
The final step in the ingest process is to register a formats-variables record. This record links pecan variables with variables from the selected data.</p>
<ul>
<li><p>Variable: PEcAn variable that is equivalent to variable in selected file.</p></li>
<li><p>Name: The variable name in the imported data need only be specified if it differs from the BETY variable name.</p></li>
<li><p>Unit: Should be in a format parseable by the udunits library and need only be secified if the units of the data in the file differ from the BETY standard.</p></li>
<li><p>Storage Type: Storage type need only be specified if the variable is stored in a format other than would be expected (e.g. if numeric values are stored as quoted character strings). Additionally, storage_type stores POSIX codes that are used to store any time variables (e.g. a column with a 4-digit year would be <code>%Y</code>).</p></li>
<li><p>Column Number: Vector of integers that list the column numbers associated with variables in a dataset. Required for text files that lack headers.
<br>
<img src="04_advanced_user_guide/02_adding_to_pecan/01_case_studies/images/data-ingest/D1Ingest-9_sm.gif" width="50%" height="50%" style="display: block; margin: auto;" /></p></li>
</ul>
<p>Finally, the path to the ingest data is displayed in the <code>Select Files</code> box.</p>
</div>
<div id="NewFormat" class="section level4">
<h4><span class="header-section-number">5.3.1.36</span> Creating a new format</h4>
<div id="formats-in-bety" class="section level5">
<h5><span class="header-section-number">5.3.1.36.1</span> Formats in BETY</h5>
<p>The PEcAn database keeps track of all the input files passed to models, as well as any data used in model validation or data assimilation. Before we start to register these files with PEcAn we need to define the format these files will be in.</p>
<p>The main goal is to take all the meta-data we have about a data file and create a record of it that pecan can use as a guide when parsing the data file.</p>
<p>This information is stored in a Format record in the bety database. Make sure to read through the current Formats before deciding to make a new one.</p>
</div>
<div id="creating-a-new-format-in-bety" class="section level5">
<h5><span class="header-section-number">5.3.1.36.2</span> Creating a new format in BETY</h5>
<p>If the Format you are looking for is not available, you will need to create a new record. Before entering information into the database, you need to be able to answer the following questions about your data:</p>
<ul>
<li>What is the file MIME type?
<ul>
<li>We have a suit of functions for loading in data in open formats such as CSV, txt, netCDF, etc.</li>
<li>PEcAn has partnered with the <a href="http://browndog.ncsa.illinois.edu/">NCSA BrownDog project</a> to create a service that can read and convert as many data formats as possible. If your file type is less common or a proprietary type, you can use the <a href="http://dap.ncsa.illinois.edu/">BrownDog DAP</a> to convert it to a format that can be used with PEcAn.</li>
<li>If BrownDog cannot convert your data, you will need to contact us about writing a data specific load function.</li>
</ul></li>
<li>What variables does the file contain?
<ul>
<li>What are the variables named?</li>
<li>What are the variable units?</li>
<li>How do the variable names and units in the data map to PEcAn variables in the BETY database? See <a href="###%20Name%20and%20Unit">below</a> for an example. It is most likely that you will NOT need to add variables to BETY. However, identifying the appropriate variables matches in the database may require some work. We are always available to help answer your questions.</li>
</ul></li>
<li>Is there a timestamp on the data?
<ul>
<li>What are the units of time?</li>
</ul></li>
</ul>
<p>Here is an example using a fake dataset:</p>
<div class="figure">
<img src="04_advanced_user_guide/images/example_data.png" alt="example_data" />
<p class="caption">example_data</p>
</div>
<p>This data started out as an excel document, but was saved as a CSV file.</p>
<p>To create a Formats record for this data, in the web interface of BETY, select Runs &gt; Formats and click <em>New Format</em>.</p>
<p>You will need to fill out the following fields:</p>
<ul>
<li>MIME type: File type (you can search for other formats in the text field)</li>
<li>Name: The name of your format (this can be whatever you want)</li>
<li>Header: Boolean that denotes whether or not your data contains a header as the first line of the data. (1 = TRUE, 0 = FALSE)</li>
<li>Skip: The number of lines above the data that should be skipped. For example, metadata that should not be included when reading in the data or blank spaces.</li>
<li>Notes: Any additional information about the data such as sources and citations.</li>
</ul>
<p>Here is the Formats record for the example data:</p>
<p><img src="04_advanced_user_guide/images/format_record_1.png" alt="format_record_1" />
When you have finished this section, hit Create. The final record will be displayed on the screen.</p>
</div>
</div>
<div id="formats---variables" class="section level4">
<h4><span class="header-section-number">5.3.1.37</span> Formats -&gt; Variables</h4>
<p>After a Format entry has been created, you are encouraged to edit the entry to add relationships between the file’s variables and the Variables table in PEcAn. Not only do these relationships provide meta-data describing the file format, but they also allow PEcAn to search and (for some MIME types) read files.</p>
<p>To enter this data, select Edit Record and on the edit screen select View Related Variable.</p>
<p>Here is the record for the example data after adding related variables:</p>
<div class="figure">
<img src="04_advanced_user_guide/images/format_record_2.png" alt="format_record_2" />
<p class="caption">format_record_2</p>
</div>
<div id="name-and-unit" class="section level5">
<h5><span class="header-section-number">5.3.1.37.1</span> Name and Unit</h5>
<p>For each variable in the file you will want at a minimum to specify the NAME of the variable within your file and match that to the equivalent Variable in the pulldown.</p>
<p>Make sure to search for your variables under Data &gt; Variables before suggesting that we create a new variable record. This may not always be a straightforward process.</p>
<p>For example bety contains a record for Net Primary Productivity:</p>
<div class="figure">
<img src="04_advanced_user_guide/images/var_record.png" alt="var_record" />
<p class="caption">var_record</p>
</div>
<p>This record does not have the same variable name or the same units as NPP in the example data.
You may have to do some reading to confirm that they are the same variable.
In this case
- Both the data and the record are for Net Primary Productivity (the notes section provides additional resources for interpreting the variable.)
- The units of the data can be converted to those of the vairiable record (this can be checked by running <code>udunits2::ud.are.convertible(&quot;g C m-2 yr-1&quot;, &quot;Mg C ha-1 yr-1&quot;)</code>)</p>
<p>Differences between the data and the variable record can be accounted for in the data Formats record.</p>
<ul>
<li>Under Variable, select the variable as it is recorded in bety.</li>
<li>Under Name, write the name the variable has in your data file.</li>
<li>Under Unit, write the units the variable has in your data file.</li>
</ul>
<p>NOTE: All units must be written in a udunits compliant format. To check that your units can be read by udunits, in R, load the udunits2 package and run <code>udunits2::is.parseable(&quot;g C m-2 yr-1&quot;)</code></p>
<p><strong>If the name or the units are the same</strong>, you can leave the Name and Unit fields blank. This is can be seen with the variable LAI.</p>
</div>
<div id="storage-type" class="section level5">
<h5><span class="header-section-number">5.3.1.37.2</span> Storage Type</h5>
<p><em>Storage Type</em> only needs to be specified if the variable is stored in a format other than what would be expected (e.g. if numeric values are stored as quoted character strings).</p>
<p>One such example is <em>time variables</em>.</p>
<p>PEcAn converts all dates into POSIX format using R functions such as <code>strptime</code>. These functions require that the user specify the format in which the date is written.</p>
<p>The default is <code>&quot;%Y-%m-%d %H:%M:%S&quot;</code> which would look like <code>&quot;2017-01-01 00:00:00&quot;</code></p>
<p>A list of date formats can be found in the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/strptime.html">R documentation for the function <code>strptime</code></a></p>
<p>Below are some commonly used codes:</p>
<table>
<colgroup>
<col width="9%" />
<col width="90%" />
</colgroup>
<thead>
<tr class="header">
<th>%d</th>
<th>Day of the month as decimal number (01–31).</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>%D</td>
<td>Date format such as %m/%d/%y.</td>
</tr>
<tr class="even">
<td>%H</td>
<td>Hours as decimal number (00–23).</td>
</tr>
<tr class="odd">
<td>%m</td>
<td>Month as decimal number (01–12).</td>
</tr>
<tr class="even">
<td>%M</td>
<td>Minute as decimal number (00–59).</td>
</tr>
<tr class="odd">
<td>%S</td>
<td>Second as integer (00–61), allowing for up to two leap-seconds (but POSIX-compliant implementations will ignore leap seconds).</td>
</tr>
<tr class="even">
<td>%T</td>
<td>Equivalent to %H:%M:%S.</td>
</tr>
<tr class="odd">
<td>%y</td>
<td>Year without century (00–99). On input, values 00 to 68 are prefixed by 20 and 69 to 99 by 19 – that is the behaviour specified by the 2004 and 2008 POSIX standards, but they do also say ‘it is expected that in a future version the default century inferred from a 2-digit year will change’.</td>
</tr>
<tr class="even">
<td>%Y</td>
<td>Year with century.</td>
</tr>
</tbody>
</table>
</div>
<div id="column-number" class="section level5">
<h5><span class="header-section-number">5.3.1.37.3</span> Column Number</h5>
<p>If your data is in text format with variables in a standard order then you can specify the Column Number for the variable. This is required for text files that lack headers.</p>
</div>
</div>
<div id="retrieving-format-information" class="section level4">
<h4><span class="header-section-number">5.3.1.38</span> Retrieving Format Information</h4>
<p>To acquire Format information from a Format record, use the R function <code>query.format.vars</code></p>
<div id="inputs-1" class="section level5">
<h5><span class="header-section-number">5.3.1.38.1</span> Inputs</h5>
<ul>
<li><code>bety</code>: connection to BETY</li>
<li><code>input.id=NA</code> and/or <code>format.id=NA</code>: Input or Format record ID from BETY
<ul>
<li>At least one must be specified. Defaults to <code>format.id</code> if both provided.</li>
</ul></li>
<li><code>var.ids=NA</code>: optional vector of variable IDs. If provided, limits results to these variables.</li>
</ul>
</div>
<div id="output" class="section level5">
<h5><span class="header-section-number">5.3.1.38.2</span> Output</h5>
<ul>
<li>R list object containing many things. Fill this in.</li>
</ul>
</div>
</div>
<div id="NewBenchmark" class="section level4">
<h4><span class="header-section-number">5.3.1.39</span> Creating a new benchmark reference run</h4>
<p>The purpose of the reference run record in BETY is to store all the settings from a run that are necessary in exactly recreating it.</p>
<p>The pecan.xml file is the home of absolutely all the settings for a particular run in pecan. However, much of the information in the pecan.xml file is server and user specific and more importantly, the pecan.xml files are stored on individual servers and may not be available to the public.</p>
<p>When a run that is performed using pecan is registered as a reference run, the settings that were used to make that run are made available to all users through the database.</p>
<p>All completed runs are not automatically registered as reference runs. To register a run, navigate to the benchmarking section of the workflow visualizations Shiny app.</p>
</div>
<div id="editing-records" class="section level4">
<h4><span class="header-section-number">5.3.1.40</span> Editing records</h4>
<ul>
<li>Models</li>
<li>Species</li>
<li>PFTs</li>
<li>Traits</li>
<li>Inputs</li>
<li>DB files</li>
<li>Variables</li>
<li>Formats</li>
<li>(Link each section to relevant Bety tables)</li>
</ul>

</div>
</div>
<div id="web-curl-submission" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Submitting Workflow from Command Line</h3>
<p>This is how you can submit a workflow from the command line through the pecan web interface. This will use curl to submit all the requireed parameters to the web interface and trigger a run.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># the host where the model should run</span>
<span class="co"># never use remote sites since you will need to pass your username/password and that WILL be stored</span>
<span class="va">hostname=</span>pecan.vm
<span class="co"># the site id where to run the model (NIWOT in this case)</span>
<span class="va">siteid=</span>772
<span class="co"># start date and end date, / need to be replaced with %2F or use - (NOT TESTED)</span>
<span class="va">start=</span>2004-01-01
<span class="va">end=</span>2004-12-31

<span class="co"># if of model you want to run, rest of section parameters depend on the model selected (SIPNET 136)</span>
<span class="va">modelid=</span>5000000002
<span class="co"># PFT selected (we should just use a number here)</span>
<span class="co"># </span><span class="al">NOTE</span><span class="co">: the square brackets are needed and will need be escaped with a \ if you call this from command line</span>
<span class="ex">pft</span>[]=temperate.coniferous
<span class="co"># initial pool condition (-1 means nothing is selected)</span>
<span class="va">input_poolinitcond=</span>-1
<span class="co"># met data</span>
<span class="va">input_met=</span>99000000006

<span class="co"># variables to collect</span>
<span class="va">variables=</span>NPP,GPP
<span class="co"># ensemble size</span>
<span class="va">runs=</span>10
<span class="co"># use sensitivity analysis</span>
<span class="va">sensitivity=</span>-1,1

<span class="co"># redirect to the edit pecan.xml file</span>
<span class="va">pecan_edit=</span>on
<span class="co"># redirect to edit the model configuration files</span>
<span class="va">model_edit=</span>on
<span class="co"># use browndog</span>
<span class="va">browndog=</span>on</code></pre>
<p>For example the following will run the above workflow. Using -v in curl will show verbose output (needed) and the grep will make sure it only shows the redirect. This will show the actual workflowid:</p>
<pre><code>curl -s -v &#39;http://localhost:6480/pecan/04-runpecan.php?hostname=pecan.vm&amp;siteid=772&amp;start=2004-01-01&amp;end=2004-12-31&amp;modelid=5000000002&amp;pft\[\]=temperate.coniferous&amp;input_poolinitcond=-1&amp;input_met=99000000006&#39; 2&gt;&amp;1 | grep &#39;Location:&#39;
&lt; Location: 05-running.php?workflowid=99000000004</code></pre>
<p>In this case you can use the browser to see progress, or use the following to see the status:</p>
<pre><code>curl -s &#39;http://localhost:6480/pecan/dataset.php?workflowid=99000000004&amp;type=file&amp;name=STATUS&#39;
TRAIT   2017-12-13 08:56:56 2017-12-13 08:56:57 DONE
META    2017-12-13 08:56:57 2017-12-13 08:57:13 DONE
CONFIG  2017-12-13 08:57:13 2017-12-13 08:57:14 DONE
MODEL   2017-12-13 08:57:14 2017-12-13 08:57:15 DONE
OUTPUT  2017-12-13 08:57:15 2017-12-13 08:57:15 DONE
ENSEMBLE    2017-12-13 08:57:15 2017-12-13 08:57:16 DONE
FINISHED    2017-12-13 08:57:16 2017-12-13 08:57:16 DONE</code></pre>
<p>Or to show the output log:</p>
<pre><code>curl -s &#39;http://localhost:6480/pecan/dataset.php?workflowid=99000000004&amp;type=file&amp;name=workflow.Rout&#39;

R version 3.4.3 (2017-11-30) -- &quot;Kite-Eating Tree&quot;
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type &#39;license()&#39; or &#39;licence()&#39; for distribution details.

R is a collaborative project with many contributors.
....</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pecan-manual-setup.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="developer-guide.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tonygardella/pecan/edit/release/vtonydoc/book_source/02_demos_tutorials_workflows/02_user_demos/01_introductions_user.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
